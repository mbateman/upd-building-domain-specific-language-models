{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9f4206",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# M1 Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f05c2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this preliminary milestone is to load and preprocess the dataset. The raw text is noisy and we want to remove nonwords and non-ASCII characters, keep punctuation to a minimum, and reduce the overall vocabulary of the corpus.\n",
    "\n",
    "- Although this corpus is not as noisy as a text directly extracted from a social network (for example, Twitter or Facebook), it is still not as structured as academic papers or newspaper articles. Furthermore, the corpus displays some interesting particularities, such as the presence of HTML markup and LaTeX-formatted equations. The corpus is also rich in specific entities, names of theorems, and statistical test algorithms, and it mixes colloquial writing with more formally structured paragraphs.\n",
    "\n",
    "\n",
    "- The garbage-in, garbage-out golden rule of machine learning is also applicable to language models. Simply put, if we skip the preprocessing/cleaning part of the project, the vocabulary of our language model will be too vast and noisy to make any sense. Generated text, for instance, may mix in mathematical symbols with punctuation signs or random HTML tags and numbers. By reducing the volume of the corpus vocabulary, we increase the relevance and quality of the generated text and improve the reliability of sentence selection based on their respective probabilities. We also reduce the memory imprint of our code and its execution time.\n",
    "\n",
    "\n",
    "- Preprocessing the text to reduce noise and vocabulary size is an iterative process. You should start simple and further refine the preprocessing steps after building and evaluating your first language models.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553d6381",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad111b6f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load the dataset into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2b0dcc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/data/stackexchange_812k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd7ce3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 812132 entries, 0 to 812131\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   post_id     812132 non-null  int64  \n",
      " 1   parent_id   75535 non-null   float64\n",
      " 2   comment_id  553076 non-null  float64\n",
      " 3   text        812132 non-null  object \n",
      " 4   category    812132 non-null  object \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 31.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bcadf5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95af6951",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498456    S. Haykin, Adaptive Filter Theory, 5th Edition...\n",
       "89243     Understanding the violation of the independenc...\n",
       "15152     Confusion over lmer and p-values: how do p-val...\n",
       "15904     Is a large control sample better than a balanc...\n",
       "170280    <p>Figure 1 there clarifies things a bit. All ...\n",
       "649371    Fair enough, I agree/stand corrected. I still ...\n",
       "112754    <p>There are some angles on this to consider. ...\n",
       "810641                  Any question @RiturajSinghRathore ?\n",
       "120845    <p>Your example is a very good one because it ...\n",
       "66418     Question about notation of expectation operato...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b201a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Use regular expressions to remove elements that are not words, such as HTML tags, LaTeX expressions, URLs, digits, and line returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981ebe2a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "HTML = \"<[^>]*>\"\n",
    "LATEX = \"\\$[^>]*\\$\"\n",
    "URLS = \"http\\S+\"\n",
    "CRS = \"[\\r\\n]+\"\n",
    "DIGITS = \"\\$[^>]*\\$\"\n",
    "SPACES = \"\\s\\s+\"\n",
    "PUNCT = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(PUNCT)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = re.sub(HTML,' ', text)\n",
    "    text = re.sub(LATEX,' ', text)\n",
    "    text = re.sub(URLS,' ', text)\n",
    "    text = re.sub(CRS,' ', text)\n",
    "    text = re.sub(DIGITS,' ', text)\n",
    "    text = re.sub(pattern,' ', text)\n",
    "    text = re.sub(SPACES,' ', text)\n",
    "    text = re.sub(DIGITS,' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2cac22d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formulate hypotheses when'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('Formulate hypotheses when $\\mu_A < \\mu_B$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece878bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See my response to a href'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('See my response to <a href=\"https://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cae6a43",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "May fit better under: http://stats.stackexchange.com/questions/1906/data-mining-conferences?\n",
      "--------------------\n",
      "$X$ is the mean of something and is distributed as $Exp(1)$ and Y is the actual value (i.e. not the mean) with parameter $X = x$ such that $Y | X = x$ is distributed as $Pois(X = x)$. I'll give my own answer right now and hopefully, you can confirm it.\n",
      "--------------------\n",
      "What result do you get if you just use a random forest regression model instead of the classifier and then regressor?\n"
     ]
    }
   ],
   "source": [
    "# Sample of comments\n",
    "for p in df[df.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4d4341",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.text = df.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6302af1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Glen b and samooch and Marius The formula expansion of factor symbol factor time will include main effects for both symbol and time. Furthermore, the 0 will only change the labeling of the effects. Instead of an Intercept term you will see the estimates for interaction of the lowest levels for symbol and time. If you wanted to avoid estimating a main effect for time you would need to use factor symbol factor time\n",
      "--------------------\n",
      "I don't understand your question. Are you asking what does it mean if all variables are most strongly correlated with PC1?\n",
      "--------------------\n",
      "Yes, I didn't know the term eigenface but that's what they are when I have just one output neuron with 6 output neurons I still get faces but they are much more noisy .\n"
     ]
    }
   ],
   "source": [
    "# Post clean sample of comments\n",
    "for p in df[df.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970bdf9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Remove texts that contain blanks only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936605ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "435cbb99",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1422"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text.str.len() == 0].text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319346b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1422 out of 812132 entries have a zero length text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d07c3cf3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df.text.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ba2c26",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810710"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace21d07",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Remove texts that are extremely large or too short to add any information to the model. \n",
    "\n",
    "We want to keep paragraphs that contain at least a few words and remove the paragraphs that are composed of large numerical tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fbdfea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "df['tokens'] = df.text.apply(lambda t : tokenizer.tokenize(t.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f036c7c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['n_tokens'] = df.tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b7e664",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    810710.000000\n",
       "mean         63.246199\n",
       "std         122.586727\n",
       "min           1.000000\n",
       "25%          16.000000\n",
       "50%          36.000000\n",
       "75%          72.000000\n",
       "max       14835.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f038227b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14835"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_tokens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b543f16",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791172, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df.n_tokens > 4) & (df.n_tokens < 5000)].reset_index(drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3274d6a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Use a tokenizer to create a version of the original text that is a string of space-separated lowercase tokens. \n",
    "\n",
    "For instance,\n",
    "\n",
    "- Thank you!, This equation y = ax + by=ax+b, is very helpful.\n",
    "\n",
    "    would be transformed to:\n",
    "\n",
    "    thank you ! this equation , is very helpful .\n",
    "\n",
    "- “retrieve a distance matrix” is a matter of coding. It also might be irrelevant: one can imagine creative answers.\n",
    "\n",
    "    becomes, if you choose to remove double quotes from the original text:\n",
    "\n",
    "    retrieve a distance matrix is a matter of coding. it also might be irrelevant : one can imagine creative answers .\n",
    "\n",
    "Note that punctuation signs (, . : !) are also represented as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6194550",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45cc342c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def space_separated_lower(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return \" \".join(list(filter(lambda x: x not in ['“', \"”\"], tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00ff51be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieve a distance matrix is a matter of coding . it also might be irrelevant : one can imagine creative answers .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '“retrieve a distance matrix” is a matter of coding. It also might be irrelevant: one can imagine creative answers.'\n",
    "space_separated_lower(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57654c1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df.text.apply(space_separated_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4afec4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Export the resulting DataFrame into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3df59f49",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "df.to_csv(\"../data/stackexchange_cleaned.csv\", quoting = csv.QUOTE_ALL, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5e15",
   "metadata": {},
   "source": [
    "# M2 N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d8539",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Objective\n",
    "\n",
    "In this second milestone of the liveProject, the objective is to build an n-gram language model that is defined by the probabilities of all the n-grams in the corpus. Assuming that each token only depends on n-1 previous tokens, the language model is fully defined by the probability of any token in the corpus given its n-1 previous tokens (the prefix).\n",
    "\n",
    "You will use the language model to complete the following tasks:\n",
    "\n",
    "Generate text and complete queries from sequences of n-grams, using temperature sampling to tune the randomness of the generated text.\n",
    "Calculate the probability of a sentence and select the most probable sentence among several candidates.\n",
    "Score the quality of the language model using perplexity.\n",
    "Handle out-of-vocabulary (OOV) tokens with Laplace smoothing.\n",
    "\n",
    "Although simple in its approach, an n-gram language model with additive smoothing is a fast and reliable way to build a model that you can exploit for simple tasks such as query completion and sentence selection, provided that the training dataset is large and specific enough for the domain of interest.\n",
    "\n",
    "This first language model will serve as a baseline for the more complex language models that we will create in subsequent tasks. It also underlines the different problems and challenges inherent to any NLP task, such as handling out-of-vocabulary tokens, the importance of cleaning the original raw data, and the quality assessment of a language model.\n",
    "\n",
    "An n-gram model is defined as the probabilities of all the n-grams in the corpus. Under certain Markovian independence assumptions, this is equivalent to evaluating the probability of any token given its n-1 previous tokens (the prefix). For a given prefix, the probabilities of all the following tokens add up to 1 and constitute the probability distribution of the prefix.\n",
    "\n",
    "For instance, in our current corpus, the prefix “how many” may be followed by the words “people,” “times,” or “ways,” with respective frequencies of 0.46, 0.31, and 0.23, while the prefix “the model” is followed by the words “parameters” or “is” or a period, with frequencies 0.43, 0.36, and 0.21, and so forth.\n",
    "\n",
    "In an n-gram language model, the probability of a token given a prefix of n-1 tokens is given by its maximum likelihood estimate (MLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544792aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set some global parameters\n",
    "\n",
    "# Displaying all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# We will work with trigrams \n",
    "ngrams_degree = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e507a",
   "metadata": {},
   "source": [
    "## Split the dataset into a training and a testing subset. \n",
    "\n",
    "Use the category “title” for the testing set and the categories “comment” and “post” for the training set. The short length of titles will make them good candidates later as seeds for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59618ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/stackexchange_cleaned.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d45b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791172, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dd0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 791172 entries, 0 to 791171\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   post_id     791172 non-null  int64  \n",
      " 1   parent_id   74519 non-null   float64\n",
      " 2   comment_id  542016 non-null  float64\n",
      " 3   text        791172 non-null  object \n",
      " 4   category    791172 non-null  object \n",
      " 5   tokens      791172 non-null  object \n",
      " 6   n_tokens    791172 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 42.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bc746d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I am working on a machine control project. We ...\n",
       "1    I have a set of sea surface temperature SST mo...\n",
       "2    This is assuming , which does not seem realist...\n",
       "3    Adolphe Quetelet for his work on the average m...\n",
       "4    Transformation of X is for linear relation bet...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4055ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda t : tokenizer.tokenize(' '.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a208d728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['iagocarvalho', 'i', 'am', 'not', 'sure', '.', 'i', 'don', \"'\", 't', 'think', 'there', 'is', 'any', '...']),\n",
       "       list(['whuber', ',', 'yes', ',', 'that', 'too']),\n",
       "       list(['i', 'don', \"'\", 't', 'see', 'the', 'commands', 'you', 'used', 'to', 'scale', 'the', 'data', '.', 'perhaps', 'there', 'is', 'an', 'error', 'there', '.', 'if', 'you', 'plot', 'each', 'scaled', 'variable', 'versus', 'its', 'corresponding', 'raw', 'variable', ',', 'you', 'should', 'get', 'nothing', 'but', 'straight', 'lines', '.', '...', 'oh', ',', 'and', 'if', 'you', 'scaled', \"'\", 'result', \"'\", 'too', ',', 'it', \"'\", 's', 'no', 'wonder', ',', 'because', 'you', \"'\", 're', 'no', 'long', 'modeling', 'the', 'sme', 'response', 'variable', '.', 'either', 'way', ',', 'we', 'need', 'more', 'details', 'on', 'what', 'you', 'mean', 'by', 'scaling', '.']),\n",
       "       list(['user112758', 'could', 'you', 'post', 'the', 'equations', 'in', 'question', '?', 'i', \"'\", 'm', 'afraid', 'i', 'don', \"'\", 't', 'have', 'the', 'book']),\n",
       "       list(['in', 'my', 'experience', 'kde', 'is', 'good', 'for', 'finding', 'critical', 'points', 'maxes', ',', 'mins', 'and', 'inflection', 'points', '.', 'you', 'can', 'get', 'those', 'on', 'one', 'pass', '--', 'but', 'only', 'after', 'you', \"'\", 've', 'come', 'up', 'with', 'a', 'suitable', 'bandwidth', '.', 'the', 'density', 'profile', 'itself', 'from', 'basic', 'kde', 'has', 'bias', 'proportional', 'to', 'the', 'second', 'curvature', 'of', 'the', 'density', 'profile', ',', 'so', 'bias', 'correction', 'involves', 'using', 'kde', 'multiple', 'times', 'to', 'estimate', 'derivatives', 'of', 'densities', '.', 'once', 'you', 'go', 'down', 'the', 'kde', 'hole', ',', 'expect', 'to', 'be', 'there', 'for', 'a', 'while', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f920ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and testing subset\n",
    "df_train = df[df.category.isin(['post', 'comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3cb316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training set: (707373, 7)\n",
      "\n",
      "   post_id  parent_id  comment_id  \\\n",
      "0    16054        NaN         NaN   \n",
      "1     9739        NaN         NaN   \n",
      "2   313248        NaN    762205.0   \n",
      "3     5596     5115.0         NaN   \n",
      "4   372739        NaN    700543.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  I am working on a machine control project. We ...     post   \n",
      "1  I have a set of sea surface temperature SST mo...     post   \n",
      "2  This is assuming , which does not seem realist...  comment   \n",
      "3  Adolphe Quetelet for his work on the average m...     post   \n",
      "4  Transformation of X is for linear relation bet...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  [i, am, working, on, a, machine, control, proj...       147  \n",
      "1  [i, have, a, set, of, sea, surface, temperatur...       253  \n",
      "2  [this, is, assuming, ,, which, does, not, seem...        12  \n",
      "3  [adolphe, quetelet, for, his, work, on, the, a...        38  \n",
      "4  [transformation, of, x, is, for, linear, relat...        37  \n",
      "\n",
      "-- Testing set (83799, 7)\n",
      "\n",
      "    post_id  parent_id  comment_id  \\\n",
      "14   252788        NaN         NaN   \n",
      "15    55807        NaN         NaN   \n",
      "20    45309        NaN         NaN   \n",
      "22   222746        NaN         NaN   \n",
      "36    16865        NaN         NaN   \n",
      "\n",
      "                                                 text category  \\\n",
      "14  Hypothesis testing t test for mean difference ...    title   \n",
      "15             Multi-armed bandit algorithms in Java?    title   \n",
      "20    Correlation between one variable and few others    title   \n",
      "22  Is it good practice to adjust for multiple com...    title   \n",
      "36  Notation conventions for random variables and ...    title   \n",
      "\n",
      "                                               tokens  n_tokens  \n",
      "14  [hypothesis, testing, t, test, for, mean, diff...        11  \n",
      "15  [multi, -, armed, bandit, algorithms, in, java...         8  \n",
      "20  [correlation, between, one, variable, and, few...         7  \n",
      "22  [is, it, good, practice, to, adjust, for, mult...        16  \n",
      "36  [notation, conventions, for, random, variables...         8  \n"
     ]
    }
   ],
   "source": [
    "# Display the dimensions of the dataframe \n",
    "print(\"-- Training set: {}\\n\".format(df_train.shape))\n",
    "# and the 1st 5 lines\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n-- Testing set {}\\n\".format(df_test.shape))\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2b7b7",
   "metadata": {},
   "source": [
    "## Build the matrix of prefix—word frequencies.\n",
    "\n",
    "- Use the ngrams function from nltk.utils to generate all n-grams from the corpus.\n",
    "\n",
    "\n",
    "- Set the following: left_pad_symbol = \\<s> and right_pad_symbol = \\</s>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30539639",
   "metadata": {},
   "source": [
    "### Counting bigrams and following tokens\n",
    "\n",
    "We build a counts object defined as a defaultdict(Counter). \n",
    "\n",
    "Taking into account all trigrams (ngrams_degree = 3) that we break into prefix (bigrams) followed by single tokens. \n",
    "\n",
    "The counts object will have the bigrams as keys and for each key a Counter of all the potential tokens. \n",
    "\n",
    "For instance, if the corpus contains a 100 instances of \"*how many people*\" and a 120 instances of \"*how many times*\" we would get the following entry:\n",
    "\n",
    "    counts[('how', 'many')] = Counter('people': 100, 'times': 120, .... )\n",
    "\n",
    "Similarly if the corpus contains \"*the model is*\" 500 times and \"*the model parameters*\" 200 times, we end up with:\n",
    "\n",
    "    counts[('the', 'model')] = Counter('is': 500, 'parameters': 200, .... )\n",
    "\n",
    "To split the tokens into bigramns we use the [ntlk.ngrams](https://www.nltk.org/api/nltk.html#nltk.util.ngrams) function:\n",
    "\n",
    "\n",
    "    Return the ngrams generated from a sequence of items, as an iterator.\n",
    "    For example:\n",
    "\n",
    "    >>> from nltk.util import ngrams\n",
    "    >>> list(ngrams([1,2,3,4,5], 3))\n",
    "    [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
    "\n",
    "The next cell should take a couple of minutes.\n",
    "\n",
    "Note that we build the mode on the training subset df_train and leave the testing subset aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a46619c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d0e1fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(sentence, ngrams_degree=3):\n",
    "    return ngrams(\n",
    "        sentence, \n",
    "        n = ngrams_degree,  \n",
    "        pad_right = True, \n",
    "        pad_left = True, \n",
    "        left_pad_symbol = \"<s>\", \n",
    "        right_pad_symbol = \"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "961d0ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', '<s>', 'the'),\n",
       " ('<s>', '<s>', 'the', 'difference'),\n",
       " ('<s>', 'the', 'difference', 'between'),\n",
       " ('the', 'difference', 'between', 'the'),\n",
       " ('difference', 'between', 'the', 'two'),\n",
       " ('between', 'the', 'two', 'approaches'),\n",
       " ('the', 'two', 'approaches', 'is'),\n",
       " ('two', 'approaches', 'is', 'discussed'),\n",
       " ('approaches', 'is', 'discussed', 'here'),\n",
       " ('is', 'discussed', 'here', '</s>')]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "list(n_grams(sentence.split(), 4 ) )[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f2928730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 705964/705964 [01:16<00:00, 9262.12it/s]\n"
     ]
    }
   ],
   "source": [
    "counts = defaultdict(Counter)\n",
    "for tokens in tqdm(df_train.tokens.values):\n",
    "    for ngram in n_grams(tokens):      \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        counts[prefix][token] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "98074d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 3332935 bigrams\n"
     ]
    }
   ],
   "source": [
    "print(\"we have {} bigrams\".format(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "88319b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chance', 'versus'): \tCounter({'actual': 1, 'probability': 1, 'systematic': 1, 'randomness': 1})\n",
      "('in', 'statmod'): \tCounter({'and': 1})\n",
      "('players', 'lurking'): \tCounter({'around': 1})\n",
      "('causational', 'research'): \tCounter({'.': 1})\n",
      "('based', 'ensemble'): \tCounter({'methods': 2, 'method': 1})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,counts[prefix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "787a131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_count = [ len(v)   for k,v in counts.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "966f44df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "plt.hist(tokens_count, bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e0fb6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839506 bigrams_with_single_tokens\n",
      "511760 bigrams_with_two_tokens\n"
     ]
    }
   ],
   "source": [
    "bigrams_with_single_tokens = [ k   for k,v in counts.items() if len(v) == 1 ]\n",
    "bigrams_with_two_tokens = [ k   for k,v in counts.items() if len(v) == 2 ]\n",
    "\n",
    "print(\"{} bigrams_with_single_tokens\".format(len(bigrams_with_single_tokens)))\n",
    "print(\"{} bigrams_with_two_tokens\".format(len(bigrams_with_two_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b12dd324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>'): 11564, ('of', 'the'): 10815}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dict = { k:len(v)   for k,v in counts.items() if len(v) > 10000 }\n",
    "tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cab2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix= ('<s>', '<s>') \n",
      "tokens= [('i', 122361), ('the', 31612), ('thanks', 22184), ('you', 22096), ('this', 17342), ('it', 16576), ('if', 16295), ('what', 14034), ('thank', 13274), ('yes', 12196)]\n",
      "707373\n"
     ]
    }
   ],
   "source": [
    "for prefix, tokens in counts.items():\n",
    "    print(\"prefix=\", prefix, \"\\ntokens=\", tokens.most_common(10))\n",
    "    print(sum(counts[prefix].values()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065c350",
   "metadata": {},
   "source": [
    "###  token / prefix probabilities\n",
    "\n",
    "To obtain token / prefix probabilities using the Maximum Likelihood Estimator, we must simply normalize each (prefix - token) count by the total number of the prefix occurence. \n",
    "\n",
    "$$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}$$\n",
    "\n",
    "\n",
    "Keeping the same defaultdict(Counter) structure for the freq object, we should obtain something similar to \n",
    "\n",
    "\n",
    "    freq[('how', 'many')] = {'people': 0.14, 'times': 120, .... }\n",
    "\n",
    "with \n",
    "* p(people / how many) = c('how many people') / c('how many') \n",
    "* p(times / how many) = c('how many times') / c('how many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6cb2c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(dict)\n",
    "for prefix, tokens in counts.items():\n",
    "    total = sum(counts[prefix].values())\n",
    "    for token, count in tokens.items():\n",
    "        freq[prefix][token] = count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b8def671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tests', 'performs'): \t{'equally': 0.3333333333333333, 'really': 0.3333333333333333, 'one': 0.3333333333333333}\n",
      "('variable', 'alongside'): \t{'several': 0.25, 'the': 0.25, 'an': 0.5}\n",
      "('philosophies', 'diverge'): \t{'many': 1.0}\n",
      "('determined', 'groups'): \t{'it': 1.0}\n",
      "('basic', 'patterns'): \t{'very': 0.5, 'don': 0.5}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prefix = random.choice(list(freq.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,freq[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5217dc",
   "metadata": {},
   "source": [
    "## Write a text generation function with the following features:\n",
    "\n",
    "- Takes a bigram as input and generates the next token\n",
    "\n",
    "\n",
    "- Iteratively slides the prefix over the generated text so that the new prefix includes the most recent token; generates the next token\n",
    "\n",
    "\n",
    "- To generate each next token, samples the list of words associated with the prefix using the probability distribution of the prefix\n",
    "\n",
    "\n",
    "- Stops the text generation when a certain number of words have been generated or the latest token is a \\</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77c00371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, n_words = 40):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        probabilities = list(freq[prefix].values())\n",
    "        text += ' ' + np.random.choice(candidates, p = probabilities)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e9b5add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'model')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple('the model'.split()[-ngrams_degree+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1cf5ebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the model for example , library , but still successful in analyzing . for more information on the previous thread at cdf raised to a normal distribution function , so here it ' s also shiny </s>\n",
      "\n",
      "that distribution , type , or a different q -- not only x property of the seasonal ma coefficients . these values are distributed over time are statistically signficantly greater than one definition of causality ? </s>\n",
      "\n",
      "to determine the lines join at s0 which may be i 1 here ? pc1 or pc2 , pt3 pc3 , and produced something different values of both groups ? in a certain application of this function assumes the data ? </s>\n"
     ]
    }
   ],
   "source": [
    "text      = 'the model'\n",
    "print()\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'that distribution'\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'to determine'\n",
    "print(generate(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec1451",
   "metadata": {},
   "source": [
    "## Write a function that can estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n",
    "\n",
    "\n",
    "Split the sentence into trigrams and use the chain rule to calculate the probability of the sentence as a product of the bigrams—tokens probabilities.\n",
    "\n",
    "- Estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n",
    "\n",
    "- Similar to the above process calculate the candidates and initial_probabilites from freq dictionary\n",
    "\n",
    "- Here we will modify the initial_probabilites using temperature and normalizing it to generate random candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba280e6",
   "metadata": {},
   "source": [
    "### Temperature sampling\n",
    "\n",
    "As you may have noticed, for some bigrams, one particular token may be much more frequent than the others potential tokens. \n",
    "\n",
    "For instance:\n",
    "\n",
    "* ('building', 'machine'): \t{'learning': 0.875, 'classification': 0.125}\n",
    "\n",
    "when generating the next token based on the bigram \"*building machine*\", most of the times the word \"learning\" will be chosen instead of \"classification\".\n",
    "\n",
    "In order to compensate these imbalances and improve the chances of less frequent tokens to be chosen we can sample with temperature.\n",
    "\n",
    "In order to increase the randomness of the next token selection given a prefix, we can flatten the distribution using the temperature $$\\tau$$ to define a new probability distribution as such:\n",
    "\n",
    "$$f_{\\tau}(p_i) = \\frac{ p_i^{\\frac{1}{\\tau}} }{ \\sum_j p_j^{\\frac{1}{\\tau}} }$$\n",
    "\n",
    "See [this post](https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling) for a more in-depth explanation on temperature sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4e50e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temp(text, temperature = 1, n_words = 30):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        initial_probabilities = list(freq[prefix].values())\n",
    "        denom   = sum([p ** temperature for p in initial_probabilities])\n",
    "        probabilities  = [p ** temperature / denom  for p in initial_probabilities]\n",
    "        text  += ' ' + np.random.choice(candidates, p = probabilities)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2da17e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "the model those ? note to myself what you represent negative intra - layer is with any application in retrieval , a confusion which is suggestive , it implicitly confuses estimation with\n",
      "0.5\n",
      "the model conditioned on these topics monte carlo approach by pesaran et al were the output contains 7 binary questions . yes value 1 0 14098 . 1 plus a column real\n",
      "1\n",
      "the model predictor predictor information is relevant to the approach i am trying to help people answer no . i can find some good opinion from you . edit yes , the\n",
      "3\n",
      "the model is not the same as the number of observations . </s>\n",
      "10\n",
      "the model . </s>\n"
     ]
    }
   ],
   "source": [
    "text  = 'the model'\n",
    "# text  = 'to determine'\n",
    "# text  = 'not sure'\n",
    "\n",
    "for tau in [0.01, 0.5, 1, 3, 10]:\n",
    "    print(tau)\n",
    "    print(generate_temp(text, temperature = tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d617432",
   "metadata": {},
   "source": [
    "## Implement the perplexity scoring function for a given sentence and for the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185cc34",
   "metadata": {},
   "source": [
    "Let's now implement a way to measure the quality of our model.\n",
    "\n",
    "The idea is to estimate the probability of a test sentence given our model. \n",
    "An uncommon sentence should be less probable than a common one.\n",
    "\n",
    "\n",
    "Notes : \n",
    "  1. At this point the sentence should exist in the corpus. Our model does not know yet how to handle out-of-vocabulary (OOV) bigrams, trigrams or tokens.\n",
    "  2. To avoid the problem of underflow caused by multiplying multiple very small floats, we work in the log space:\n",
    "\n",
    "So instead of calculating perplexity with (case ngrams_degree = 3):\n",
    " \n",
    "$$PP(w_{1},\\cdots, w_N) = ( \\prod_{i = 3}^{N} \\frac{1}{ p(w_i/ w_{i-2}w_{i-1} )} )^{\\frac{1}{N}}$$\n",
    "\n",
    "We compute\n",
    "\n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 3}^{N} \\log {p(w_i/ w_{i-2}w_{i-1}} } ) ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b626ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def perplexity(sentence):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "\n",
    "    for ngram in n_grams(sentence): \n",
    "        try:\n",
    "            prefix = ngram[:ngrams_degree-1] \n",
    "            token = ngram[ngrams_degree-1]\n",
    "            logprob += np.log(freq[prefix][token])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.exp(- logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "de11a326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 22.95] the difference between the two approaches is discussed here\n",
      "\n",
      "[perplexity 38.30] this question really belongs on a different site\n",
      "\n",
      "[perplexity 73.33] The function may only be linear in the region where the points were taken\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"The function may only be linear in the region where the points were taken\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eca363",
   "metadata": {},
   "source": [
    "## Implement additive Laplace smoothing to give a non-zero probability to missing prefix—token combinations when calculating perplexity.\n",
    "\n",
    "### Out of Vocabulary (OOV) \n",
    "\n",
    "The main weakness of our model so far is that it does not know how to handle elements that are not already in the original corpus.\n",
    "\n",
    "Since both when generating text and when calculating perplexity we use the count of the prefix in the corpus, when that prefix is missing, the counts = 0  which causes problems with logs and divisions.\n",
    "\n",
    "To remediate to that problem we can artificially assign a probability (although a very low one) to missing ngrams and tokens.\n",
    "\n",
    "This method is called Laplace smoothing. It relies on calculating the frequency of a token / prefix with:\n",
    "\n",
    "$$ p(token / prefix) = \\frac{ count( prefix + token) + \\delta}{count(prefix) + \\delta \\times |N| }$$\n",
    "\n",
    "\n",
    "Where \n",
    "\n",
    "* N is the total number of prefixes in the model\n",
    "* delta is an arbitrary number \n",
    "\n",
    "When the prefix is missing from the original corpus, the probability of a token / prefix will now be:\n",
    "\n",
    "$$p(token / prefix) = \\frac{1} { | N |}$$\n",
    "\n",
    "Let's implement that perplexity with Laplace Smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e74d3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_laplace(sentence, delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    for ngram in n_grams(sentence): \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        if prefix in list(counts.keys()):\n",
    "            total = sum(counts[prefix].values())\n",
    "            if token in counts[prefix].keys():\n",
    "                logprob += np.log((counts[prefix][token] + delta)/ (total + delta * N))\n",
    "            else:\n",
    "                logprob += np.log((delta) / (total + delta * N ))\n",
    "        else:\n",
    "            logprob += - np.log(N)\n",
    "  \n",
    "    return np.exp(- logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cc61c65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 145.18] this model belongs on a different planet\n",
      "[perplexity 35.54] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity of sentences that were not present in the original corpus.\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bae00",
   "metadata": {},
   "source": [
    "## Calculate the perplexity of the language model on the test set composed of titles.\n",
    "\n",
    "Perplexity on the test corpus and sentence probability.\n",
    "\n",
    "How do we calculate the perplexity of a model on a test corpus?\n",
    "\n",
    "Let's say we have *m* sentences in the corpus, the perplexity of the corpus is given by \n",
    "\n",
    "$$ PP(Corpus) = P(S_1, \\cdots, S_m)^{-\\frac{1}{N}} $$\n",
    "\n",
    "We can assume that the sentences are independent\n",
    "\n",
    "$$ PP(Corpus) = (\\prod_{k = 1}^{m}  P(S_k))^{-\\frac{1}{N}} $$\n",
    "\n",
    "Which we calculate in the log space to avoid underflow\n",
    "\n",
    "$$ PP(Corpus) = \\exp ( -\\frac{1}{N} \\sum_{k = 1}^{m}  log(P(S_k)) $$\n",
    "\n",
    "So to calculate the perplexity on a test corpus we need to calculate the probability of each single sentence.\n",
    "\n",
    "The following function calculates the probability of a sentence. \n",
    "\n",
    "Instead of using laplace smoothing to deal with the missing bigrams and tokens, we will simply skip missing elements to make the function faster.\n",
    "Implementing laplace smoothing requires several extra conditions that are taking too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c2c36af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_probability(sentence, delta = 1, ngrams_degree = 3):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    logprob = 0\n",
    "    for ngram in n_grams(sentence, ngrams_degree):\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        try:\n",
    "            logprob += np.log( freq[prefix][token] )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "81a1c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus, ngrams_degree = 3):\n",
    "    # start by calculating the total number of tokens in the corpus\n",
    "    all_sentences = ' '.join(corpus)\n",
    "\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    N = len(tokens)\n",
    "\n",
    "    logprob = 0\n",
    "    probs = []\n",
    "    for sentence in tqdm(corpus):\n",
    "        lp = sentence_log_probability(sentence, ngrams_degree)\n",
    "        probs.append(lp)\n",
    "        if lp != np.inf:\n",
    "            logprob += lp\n",
    "        else:\n",
    "            print(lp)\n",
    "#     print(probs)        \n",
    "    print(logprob, N)\n",
    "    return np.exp( - logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e10e9065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 10044.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31728.947433616246 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The perplexity of a sample of 1000 titles\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "593dbd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83799/83799 [00:02<00:00, 32080.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2671335.5473894435 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the perplexity of the whole test corpus\n",
    "corpus_perplexity(df_test.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22982ac3",
   "metadata": {},
   "source": [
    "## Try to improve the perplexity score of your model as follows:\n",
    "\n",
    "- Modify the preprocessing phase of the corpus.\n",
    "\n",
    "\n",
    "- Increase or decrease the number of tokens in the model (bigrams, 4-grams, and so on).\n",
    "\n",
    "\n",
    "- Vary the delta parameter in the additive Laplace smoothing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf0978",
   "metadata": {},
   "source": [
    "### Modify the preprocessing phase of the corpus.\n",
    "\n",
    "Not clear what should change, so not tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696ab53",
   "metadata": {},
   "source": [
    "### Increase or decrease the number of tokens in the model (bigrams, 4-grams, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b92e958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 19488.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-32098.052657143264 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The perplexity of a sample of 1000 titles and using 4-grams\n",
    "# original 3-gram score was -31728.947433616246\n",
    "# did perplexity increase?\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus, ngrams_degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cbac2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83685/83685 [00:02<00:00, 32455.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2661949.885555473 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the perplexity of the whole test corpus using 4-grams\n",
    "# original 3-gram score was -2671335.5473894435\n",
    "# did perplexity decrease?\n",
    "corpus_perplexity(df_test.text.values, ngrams_degree=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675108e",
   "metadata": {},
   "source": [
    "### Vary the delta parameter in the additive Laplace smoothing step.\n",
    "\n",
    "Perplexity using delta value of 10:\n",
    "\n",
    "[perplexity 145.18] this model belongs on a different planet\n",
    "\n",
    "[perplexity 35.54] this question really belongs on a different site.\n",
    "\n",
    "Below using a delta value of 20 seems to reduce perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c69c6223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 110.46] this model belongs on a different planet\n",
      "[perplexity 33.31] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity of sentences that were not present in the original corpus.\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 20), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 20), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a21ff1",
   "metadata": {},
   "source": [
    "## Building an n-gram language model using NLTK\n",
    "\n",
    "Since version 3.4 the nltk library includes a language model module.\n",
    "\n",
    "Let's install the right version of nltk. Feel free to install any version > 3.4.5. \n",
    "\n",
    "After running the pip install command below you will need to restart the runtime. This will erase all the local variables. So we will reload and prepare the dataset from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57bd149e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72dca2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "ngrams_degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baebb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe, shuffle it and reset the index\n",
    "# ../data/stackexchange_812k.csv\n",
    "# ../data/stackexchange_cleaned.csv\n",
    "df = pd.read_csv('../data/stackexchange_812k.tokenized.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a35f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda txt : txt.split())\n",
    "df_train = df[df.category.isin(['post','comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c68acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cca179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50543343"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(list(vocab)) = 50543343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2839d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = MLE(ngrams_degree,vocabulary=Vocabulary(unk_cutoff = 20))\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(ngrams_degree, df_train.tokens.values)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7f2b6",
   "metadata": {},
   "source": [
    "Then you can use the perplexity and generate functions of the lm module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aad6db0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.52360717347284"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perplexity(ngrams(df_test.tokens.values[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6ef694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 548.95] the difference between the two approaches is discussed here\n",
      "\n",
      "[perplexity 481.30] this question really belongs on a different site\n",
      "\n",
      "[perplexity 515.28] The function may only be linear in the region where the points were taken\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(model.perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(model.perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"The function may only be linear in the region where the points were taken\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(model.perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b0500",
   "metadata": {},
   "source": [
    "### Signature: model.generate(num_words=1, text_seed=None, random_seed=None)\n",
    "\n",
    "Generate words from the model.\n",
    "\n",
    ":param int num_words: How many words to generate. By default 1.\n",
    "\n",
    ":param text_seed: Generation can be conditioned on preceding context.\n",
    "\n",
    ":param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
    "makes the random sampling part of generation reproducible.\n",
    "\n",
    ":return: One (str) word or a list of words generated from model.\n",
    "\n",
    "Examples:\n",
    "\n",
    "\\>>> from nltk.lm import MLE\n",
    "\n",
    "\\>>> lm = MLE(2)\n",
    "\n",
    "\\>>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
    "\n",
    "\\>>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
    "\n",
    "\\>>> lm.generate(random_seed=3)\n",
    "\n",
    "\\'a'\n",
    "\n",
    "\\>>> lm.generate(text_seed=['a'])\n",
    "\n",
    "\\'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "43b249b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'university', '-', 'but', 'this', 'paper', 'it', 'doesn', \"'\", 't']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(num_words=10, random_seed=2, text_seed=['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41d36410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the model p will approach . the r package ? i have no control group ? it wouldn ' t know how calculate a mixed effects model with multiple opportunities to answer it , is zero at all that said , with\n",
      "\n",
      "that distribution weka smo looks like this one is different from the original data are incapable of understanding where the leap from ? </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "\n",
      "to determine y xj ? </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    }
   ],
   "source": [
    "random_seed=2\n",
    "n_words=40\n",
    "text      = 'the model'\n",
    "print()\n",
    "print(text+' '+' '.join(model.generate(n_words,text_seed=text,random_seed=random_seed)))\n",
    "\n",
    "print()\n",
    "text      = 'that distribution'\n",
    "print(text+' '+' '.join(model.generate(n_words,text_seed=text,random_seed=random_seed)))\n",
    "\n",
    "print()\n",
    "text      = 'to determine'\n",
    "print(text+' '+' '.join(model.generate(n_words,text_seed=text,random_seed=random_seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fad75",
   "metadata": {},
   "source": [
    "# M3 Deep Learning Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1e879",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this milestone, we will build a language model using a long short-term memory (LSTM) neural network. The problem is framed as a multiclass classification problem where the number of classes corresponds to the size of the vocabulary. The number can be quite large. Given a sequence of n-grams, the classifier predicts the following token as a class. The input of the neural network is an array of sequences of tokens for the design matrix and the output is a vector of labels that corresponds to the target token.\n",
    "\n",
    "When the vocabulary size is too large, training the model takes too long and the performance degrades. The challenge, therefore, lies in finding the right balance between the feasibility of the task and the quality of the model by reducing the vocabulary size but preserving its diversity.\n",
    "\n",
    "Your goal is to create a language model that generates high-quality text with a low perplexity score on a validation set and is reasonably fast to train.\n",
    "\n",
    "Creating a deep learning token-based language model brings specific challenges. Successfully implementing and training such a model on a real-world dataset requires the following:\n",
    "\n",
    "- Optimizing Python structures and control flows to minimize memory impact and reduce processing times\n",
    "\n",
    "- Balancing data diversity and dataset reduction\n",
    "\n",
    "\n",
    "There are many parameters to handle both in the data processing and model fitting phases, and finding the right balance is also a challenge.\n",
    "\n",
    "The language model building approach is entirely different from the n-gram approach. Instead of estimating tokens’ probability distributions using a maximum likelihood approach (counting occurrences of tokens) as we did for the n-grams language model, we train a classification model using a recurrent neural network approach.\n",
    "\n",
    "There will be a comparison of the n-gram language model built in the previous milestone with this deep learning model, highlighting the strengths, weaknesses, and difficulties inherent to each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153c9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "# Note that we will not use keras tokenizer but keep using the same NLTK tokenizer from task 1\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# We use Keras here for simplicity. Replace with your neural network of choice.\n",
    "\n",
    "#Load Keras libraries\n",
    "\n",
    "# dataframe display option\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42e49a",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f02352",
   "metadata": {},
   "source": [
    "### Load the dataset that was prepared in Milestone 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731fe836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup variables\n",
    "POSTS_TYPE = 'post'\n",
    "MIN_TOKEN_LENGTH = 100\n",
    "MAX_TOKEN_LENGTH = 200\n",
    "DF_SAMPLE_COUNT = 20000\n",
    "\n",
    "TOKENS_MIN_COUNT = 10\n",
    "\n",
    "SEQUENCE_WINDOW = 4\n",
    "SEQUENCE_LEN = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f99a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('../data/stackexchange_812k.tokenized.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17deedb7",
   "metadata": {},
   "source": [
    "### The original dataset is too large and needs to be reduced. To reduce it, you can, for instance, use the following techniques:\n",
    "\n",
    "- Filter out items that have too many or too few tokens.\n",
    "\n",
    "- Select items of a certain type, such as posts, comments, or titles.\n",
    "\n",
    "- Sub-sample items randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9378ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>789649.000000</td>\n",
       "      <td>74508.000000</td>\n",
       "      <td>540587.000000</td>\n",
       "      <td>789649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>206485.982393</td>\n",
       "      <td>184803.547002</td>\n",
       "      <td>403119.118018</td>\n",
       "      <td>61.535119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>130373.266753</td>\n",
       "      <td>132314.071250</td>\n",
       "      <td>243171.839006</td>\n",
       "      <td>93.283795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>88976.000000</td>\n",
       "      <td>64171.000000</td>\n",
       "      <td>186790.500000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>200465.000000</td>\n",
       "      <td>168181.500000</td>\n",
       "      <td>398419.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>318109.000000</td>\n",
       "      <td>296199.000000</td>\n",
       "      <td>616542.000000</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>439822.000000</td>\n",
       "      <td>439761.000000</td>\n",
       "      <td>819187.000000</td>\n",
       "      <td>4903.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             post_id      parent_id     comment_id       n_tokens\n",
       "count  789649.000000   74508.000000  540587.000000  789649.000000\n",
       "mean   206485.982393  184803.547002  403119.118018      61.535119\n",
       "std    130373.266753  132314.071250  243171.839006      93.283795\n",
       "min         1.000000       1.000000       1.000000       5.000000\n",
       "25%     88976.000000   64171.000000  186790.500000      17.000000\n",
       "50%    200465.000000  168181.500000  398419.000000      36.000000\n",
       "75%    318109.000000  296199.000000  616542.000000      72.000000\n",
       "max    439822.000000  439761.000000  819187.000000    4903.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9eaec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:  (20000, 7)\n",
      "[\"Two things First, you say you're looking at accuracy on the training sample. This will mislead you. You need to measure on a hold-out test set or do cross-validation, etc . Being more accurate on the training sample can be very misleading, since it may be due to overfitting, which will cause problems outside of the training set. Generalization has to do with data not in the training set. And the bottom line is if your training is going to be less sensitive to outliers, it's going to be less accurate on outliers -- perhaps way less accurate . Second, you say accuracy and by that I assume you mean Accuracy as in categorized correctly. But in most real problems, you should be concerned about other measures such as Precision or Recall.\"\n",
      " 'As Marsupial wrote in the comments There is a good way to find out, which is to try regularization and see if it gives better out of sample prediction when the regularization parameter is tuned properly e.g. by minimizing PRESS . I expect there are some problems with those dimensions where an unregularized liner model is fine and others where regularization is beneficial. As finding out is computationally inexpensive, why not just try it and see? I tend to use regularised models for most problems, if the regularisation term is unhelpful, you tend to end up with the regularisation parameter taking on a small value and the regularisation term has little effect.']\n"
     ]
    }
   ],
   "source": [
    "df = df_full[\n",
    "            (df_full.category == POSTS_TYPE) & \n",
    "            (df_full.n_tokens > MIN_TOKEN_LENGTH)  & \n",
    "            (df_full.n_tokens < MAX_TOKEN_LENGTH)\n",
    "        ].sample(DF_SAMPLE_COUNT).reset_index(drop = True)\n",
    "\n",
    "print(\"df.shape: \", df.shape)\n",
    "print(df.text.sample(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "547436f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['if', 'you', 'are', 'only', 'looking', 'for', 'a', 'score', 'you',\n",
      "       'can', 'take', 'inspiration', 'from', 'the', 'question', 'how',\n",
      "       'does', 'one', 'measure', 'the', 'non', '-', 'uniformity', 'of',\n",
      "       'a', 'distribution', '?', '.', 'if', 'you', 'have', 'a', 'perfect',\n",
      "       'mixing', ',', 'the', 'distribution', 'of', 'your', 'languages',\n",
      "       'should', 'be', 'uniform', 'in', 'every', 'city', 'i', '.', 'e',\n",
      "       '.', 'each', 'language', 'group', '.', 'if', 'mixing', 'is',\n",
      "       'imperfect', ',', 'it', 'will', 'not', 'be', 'uniform', '.', 'the',\n",
      "       'answer', 'to', 'this', 'post', 'suggests', 'using', 'the',\n",
      "       'metric', ',', 'the', 'entropy', 'or', 'the', 'kullback', '-',\n",
      "       'leibler', 'divergence', '.', 'i', 'would', 'actually', 'use',\n",
      "       'the', 'last', 'one', ',', 'which', 'easily', 'allows', 'you',\n",
      "       'to', 'normalize', 'for', 'unequal', 'language', 'distribution',\n",
      "       'in', 'the', 'whole', 'population', '.'], dtype='<U12')]\n"
     ]
    }
   ],
   "source": [
    "# transform the tokens field from white space separated strings into list of tokens\n",
    "df['tokens'] = df.tokens.apply(lambda t : np.array(t.split()))\n",
    "print(df.tokens.sample().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce85110",
   "metadata": {},
   "source": [
    "### Build the vocabulary as the set of all unique tokens to construct the list of token indexes.\n",
    "\n",
    "Filtering on token frequency is one way to reduce the overall size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ff3cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number of tokens 2863274\n",
      "original vocab_size 33046\n",
      "new number of tokens 2799328\n",
      "new vocab_size 7219\n"
     ]
    }
   ],
   "source": [
    "#generate vocabulary\n",
    "#filter out words that are too scarce\n",
    "import itertools\n",
    "all_tokens = list(itertools.chain.from_iterable(df.tokens))\n",
    "\n",
    "#filter out least common tokens\n",
    "from collections import Counter\n",
    "counter_tokens = Counter(all_tokens)\n",
    "\n",
    "vocab_size  = len(set(all_tokens))\n",
    "vocab       = list(set(all_tokens))\n",
    "print(\"original number of tokens\", len(all_tokens))\n",
    "print(\"original vocab_size\", vocab_size)\n",
    "\n",
    "#remove all tokens that appear in less than TOKENS_MIN_COUNT times\n",
    "fltrd_tokens = [ token for token in all_tokens if counter_tokens[token] > TOKENS_MIN_COUNT ]\n",
    "\n",
    "print(\"new number of tokens\", len(fltrd_tokens))\n",
    "print(\"new vocab_size\", len(set(fltrd_tokens)))\n",
    "\n",
    "vocab_size  = len(set(fltrd_tokens))\n",
    "vocab       = list(set(fltrd_tokens))\n",
    "vocab.append('UNK')\n",
    "vocab_size +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dddcdc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejected tokens\n",
    "rejected_tokens =  [ token for token in all_tokens if counter_tokens[token] <= TOKENS_MIN_COUNT ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de791fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rejected_tokens):  62863\n",
      "['feautres' 'interchangeably' 'nelsen' 'gpus' 'rtexttools' '╘═════╧═════╛'\n",
      " 'receptionist' 'offs' 'advices' 'dbeta' 'thins' 'nonconstant' 'cjf'\n",
      " 'homophily' 'rept' 'slack' 'floats' 'gzlm' 'standarized'\n",
      " 'randomforestclassifier' 'mayer' 'corporate' 'inaccuracy' 'itemset'\n",
      " 'apha' 'timespan' 'lahiri' 'winsorized' 'accomodate' 'briefing' 'uruguay'\n",
      " 'samuels' 'euler' 'knight' 'ripley' 'painful' 'maleheight' 'edelman'\n",
      " 'mackinnon' 'haug' 'acast' 'francois' 'magnetic' 'equivariant' 'varx'\n",
      " 'sluo' '≥' 'grandma' 'conviction' 'lizard' 'awareness' 'unduly' 'whist'\n",
      " 'hypotesis' 'formatting' 'atlantic' 'lieu' 'fam' 'neighboring' 'whith'\n",
      " 'clears' 'inevitably' 'io' 'dreadfully' 'objectives' 'pgmm' 'prestige'\n",
      " 'denoting' 'imagery' 'deparse' 'explainable' 'dampening' 'zcta' 'timeeve'\n",
      " 'responders' 'algorithmes' 'grammatical' 'micombine' 'parameterized'\n",
      " 'brazil' 'boostrap' 'thres' 'payout' 'tones' 'isolated' 'lasts'\n",
      " 'contentious' 'anatomic' 'pedagogical' 'holistic' 'tighter'\n",
      " 'identicalness' 'rearrangement' 'bulla' 'bieber' 'jrss' 'expansions'\n",
      " 'crosses' 'judgements' 'bidder']\n"
     ]
    }
   ],
   "source": [
    "print(\"len(rejected_tokens): \", len(rejected_tokens))\n",
    "print(np.random.choice(rejected_tokens, 100, replace = False))\n",
    "# len(rejected_tokens):  25497"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf994e7",
   "metadata": {},
   "source": [
    "### Set a fixed sequence length and build sequences of token indexes from the corpus. (See, for instance, Keras pad_sequences.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeeb1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('UNK')\n",
    "vocab_size +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2acb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { w : i for i, w in enumerate(vocab) }\n",
    "\n",
    "def getidx(token):\n",
    "    try:\n",
    "        return mapping[token]\n",
    "    except:\n",
    "        return mapping['UNK']\n",
    "\n",
    "df['tokens_idx'] = df.tokens.apply(lambda tokens : np.array([getidx(token) for token in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357143cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([3705,  235, 2551, 4861, 2902, 3981, 7057, 5350, 5625, 1395,  799,\n",
      "       5352, 6559, 5271, 3826, 7028,  659, 5271, 4670, 6108, 4944, 5271,\n",
      "       5625, 7064, 4861, 1095, 5623,  144, 3705, 2889, 3981, 3291, 6065,\n",
      "       2235, 4348, 5271, 1671, 5175,  769, 3981, 6332, 2708, 6415, 5271,\n",
      "       5482, 1654, 6865, 4857, 3670, 4482, 7064, 4358, 1889, 4861, 5271,\n",
      "       2015, 4861, 3885, 5271, 3305, 3853, 6921, 7064, 4358, 3161,  113,\n",
      "       4861, 5271, 6332, 2708, 7064, 5752,  357, 7201, 4561, 3305, 4861,\n",
      "       5271, 4969, 7102, 6559, 5288, 4132, 1240, 5271, 1023, 2098, 3705,\n",
      "        235, 2551, 4861, 4670, 1363,  768,  368, 3388,  673, 3305, 7064,\n",
      "       5271, 1483, 6699, 5271, 3291, 3425, 6176, 3262, 1863, 7064, 1619,\n",
      "        315,  144, 1124, 3461,  113, 1863, 7064, 4177, 7064, 5415,  144,\n",
      "       7064,  144, 1124, 3461, 6065, 1863, 7064, 3484, 1863, 7064, 1545,\n",
      "       1863, 7064, 4948, 3262,  113,  144, 7064, 7220, 4945, 3705, 3246,\n",
      "        862, 3263, 3853,  447, 5819, 4229, 4861,  342, 5271, 4670, 6108,\n",
      "        144, 6144, 2133, 5271, 5108, 6957, 5271, 3291, 6065, 6415,  113,\n",
      "       1395, 1659,  113,  144,  659, 3705,  768, 2918,  223, 5421, 6616,\n",
      "       4238, 3770, 6771, 2836, 2041, 4766, 4944, 5271, 6291, 5415, 2992,\n",
      "       4861, 6109, 5271, 2992, 1821, 7220, 1395, 7220, 1696])\n",
      " array([ 472, 6147, 3770, 5442,  327, 3425, 6176, 6204, 5476, 7064, 6204,\n",
      "       6065,  163, 5347, 3374, 4482,  144,  659, 6415, 5704, 5476,  144,\n",
      "       1234, 3374, 4482, 4238, 7220, 7064, 3981, 7220, 2041, 3966, 6176,\n",
      "       6089, 5352, 7006, 4906, 6664, 7198, 4670, 2916, 4418,  447,  227,\n",
      "       6530,  144,  144,  144,  144,  144,  144,  144,  144,  144,  144,\n",
      "        144,  144,  144,  144,  144,  144, 1654, 7220, 2916, 3165, 2041,\n",
      "       3654, 1647, 5352, 4206, 3966,  414, 3477,  144,  659, 5623, 4238,\n",
      "       5352, 4129, 5122, 7064, 5271, 2450, 5350, 5164, 6699,  327, 3425,\n",
      "       6176, 4238, 2909, 5271, 7040, 5232, 7064, 3981, 6285, 2041, 6473,\n",
      "       3966, 6065, 6451, 7220, 1236, 7064, 4572, 2916, 5421,  472, 5689,\n",
      "       4861,  401, 5271, 1118, 7188, 4568,  144,  472, 1647, 4704,  604,\n",
      "       6415, 4329, 7028, 1240, 5271, 5027, 3497, 4238,  300, 6144, 5271,\n",
      "       2821, 2041, 5271, 2992,  144, 5352, 5271, 1118, 7064, 3705, 6321,\n",
      "       7220, 3966, 4568, 1588, 7064, 6132, 5205, 5824, 5623, 4348, 7220,\n",
      "        144, 7220, 1395, 4857, 1779, 7220,  144,  144,  408, 5865, 2041,\n",
      "       5350, 7064])]\n"
     ]
    }
   ],
   "source": [
    "print(df.tokens_idx.head(2).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ee3af",
   "metadata": {},
   "source": [
    "### Split the sequences into predictors and labels (keras.utils.to_categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e83106b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most likely using tf version 2.9 - go back to 2.8 and original path works\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60cea204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [[1], [2, 3], [4, 5, 6]]\n",
    "pad_sequences(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d372de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences\n",
    "def generate_sequences(sentence):\n",
    "    sequences = []\n",
    "    _end = SEQUENCE_WINDOW\n",
    "    while _end < len(sentence) + SEQUENCE_WINDOW:\n",
    "        sequences.append(sentence[:_end])\n",
    "        _end += SEQUENCE_WINDOW\n",
    "    padded_seqs = pad_sequences(sequences, maxlen=SEQUENCE_LEN, padding='pre')\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61d73a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sequence generation \n",
    "multi_sequences = df.tokens_idx.apply(generate_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69ecdcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:47<00:00, 420.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sequences.shape:  (723428, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for d in tqdm(multi_sequences.values):\n",
    "    if i == 0:\n",
    "        all_sequences = d\n",
    "    else:\n",
    "        all_sequences = np.concatenate( ( all_sequences, d )  )\n",
    "    i +=1\n",
    "print(\"\\nsequences.shape: \",all_sequences.shape)\n",
    "# expected sequences.shape:  (722881, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9050850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample N% of the sequences to reduce the input dataset.\n",
    "if True:\n",
    "    mask = np.random.choice([False, True], len(all_sequences), p=[0.50, 0.50])\n",
    "    sequences = all_sequences[mask].copy()\n",
    "else:\n",
    "    sequences = all_sequences.copy()\n",
    "    print(\"\\nsequences.shape: \",sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "807d891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the sequences into predictors and labels\n",
    "# tf.keras.utils.to_categorical\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91d11c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical([0, 1, 2, 3], num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b46d99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictors.shape (361743, 12)\n",
      "label.shape (361743,)\n",
      "label_cat.shape (361743, 7221)\n"
     ]
    }
   ],
   "source": [
    "# create the predictors and labels for the classificaton task.\n",
    "predictors  = sequences[:,:-1]\n",
    "label       = sequences[:,-1]\n",
    "\n",
    "print(\"predictors.shape\", predictors.shape)\n",
    "print(\"label.shape\", label.shape)\n",
    "\n",
    "# The to_categorical Keras function transforms the vocab_size vector of labels into a one hot encoded matrix of dimension (n, vocab_size)\n",
    "label_cat       = to_categorical(label, num_classes=vocab_size)\n",
    "\n",
    "print(\"label_cat.shape\", label_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e116b2",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "The data is now ready to be used to fit a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807597ee",
   "metadata": {},
   "source": [
    "### Define a simple sequential model with an embedding layer, LSTM(s), and a dense layer with softmax activation. \n",
    "\n",
    "Feel free to experiment with dropouts and different optimizers. Here, we would be focusing on Keras to perform language modeling in this milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a36b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43714188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6200151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cae3fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:32:41.179591: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-11-18 18:32:41.179638: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: michael-LabTop-Pro\n",
      "2022-11-18 18:32:41.179644: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: michael-LabTop-Pro\n",
      "2022-11-18 18:32:41.179726: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 465.19.1\n",
      "2022-11-18 18:32:41.179750: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1\n",
      "2022-11-18 18:32:41.179756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 465.19.1\n",
      "2022-11-18 18:32:41.180159: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 12, 64)            462144    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 12, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7221)              469365    \n",
      "=================================================================\n",
      "Total params: 1,079,733\n",
      "Trainable params: 1,079,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define model\n",
    "an embedding dimension (32, 64, ...), \n",
    "2 LSTM layers \n",
    "followed by a dense layer with softmax activation\n",
    "the optimizer is RMSprop with a learning rate of 0.01\n",
    "'''\n",
    "embedding_dimension = 64\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(vocab_size,\n",
    "        embedding_dimension,\n",
    "        input_length=SEQUENCE_LEN -1)\n",
    "    )\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52991ad",
   "metadata": {},
   "source": [
    "### Specify the number of epochs, the batch size, and other fitting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46674209",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0715b62",
   "metadata": {},
   "source": [
    "### Fit the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a700c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:32:53.294908: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 10448584812 exceeds 10% of free system memory.\n",
      "2022-11-18 18:32:59.173526: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1414/1414 [==============================] - 62s 42ms/step - loss: 5.5694 - accuracy: 0.1400\n",
      "Epoch 2/4\n",
      "1414/1414 [==============================] - 60s 43ms/step - loss: 5.3461 - accuracy: 0.1862\n",
      "Epoch 3/4\n",
      "1414/1414 [==============================] - 61s 43ms/step - loss: 5.2892 - accuracy: 0.1996\n",
      "Epoch 4/4\n",
      "1414/1414 [==============================] - 60s 43ms/step - loss: 5.2514 - accuracy: 0.2054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf40158d50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Model Fitting!\n",
    "'''\n",
    "model.fit(predictors, label_cat, batch_size = batch_size, epochs=epochs, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e458cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb73de",
   "metadata": {},
   "source": [
    "## Assessing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fccd9",
   "metadata": {},
   "source": [
    "### Write a function that generates text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "590ba117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to sample an index from a probability array\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68f87721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(nmax, text, temperature):\n",
    "    n = 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    while (len(tokens) < nmax) :\n",
    "        n +=1\n",
    "        # only takes known words into account\n",
    "        tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "        # print(tokens_idx)\n",
    "        tokens_list = pad_sequences([tokens_idx], maxlen=SEQUENCE_LEN-1, padding='pre')\n",
    "        probas = model.predict(tokens_list, verbose=0)[0]\n",
    "        next_word_idx = sample(probas, temperature = temperature)\n",
    "        next_word = vocab[next_word_idx]\n",
    "        # print(next_word_idx, next_word)\n",
    "\n",
    "        # next_word = np.random.choice(vocab, p = probas)\n",
    "        if next_word != '?':\n",
    "            print(next_word, probas[vocab.index(next_word)]  )\n",
    "            text += ' ' + next_word\n",
    "        # print(text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if n> 200:\n",
    "            break;\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a432b",
   "metadata": {},
   "source": [
    "### Generate some text and take note of the following:\n",
    "- Token repetitions\n",
    "- Missing punctuation\n",
    "- Other anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5ab92b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intuition 7.272789e-07\n",
      "called 0.0002907602\n",
      "pretty 0.0004295586\n",
      "big 0.00025389093\n",
      "considering 2.379584e-07\n",
      "participants 2.5674164e-05\n",
      "visualization 4.543845e-06\n",
      "filter 1.6875334e-05\n",
      "date 0.00014399817\n",
      "advisable 1.2059003e-07\n",
      "statisticians 5.3037573e-05\n",
      "english 4.5822644e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a random variable intuition called pretty big considering participants visualization filter date advisable statisticians english'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(15, 'a random variable', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448f3a7",
   "metadata": {},
   "source": [
    "### Write a function that calculates the perplexity of a sentence and apply it to a subset of sentences to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29350404",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_WINDOW = 1\n",
    "\n",
    " # and define the perplexity for a sentence\n",
    "\n",
    "def perplexity(sentence):\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(tokens)\n",
    "    # find the indexes of the tokens from the vocabulary\n",
    "    tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "    # generate a N x SEQUENCE_LEN array of padded sequences \n",
    "    sequences = generate_sequences(tokens_idx)\n",
    "    predictors  = sequences[:,:-1]\n",
    "    label       = sequences[:,-1]\n",
    "    # the probabilities of all the words in the vocab given each padded sequence\n",
    "    probas = model.predict(predictors, verbose=0)\n",
    "    # add the log of the probability of the label given the padded sequence\n",
    "    logprob = 0\n",
    "    for k in range(N):\n",
    "        p = probas[k,label[k]]\n",
    "        logprob += np.log( p  )    \n",
    "    return np.exp(- logprob / N), logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43f35efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a fixed-effects model only time-varying variables can be used. (77.32176156345228, -65.21963155269623)\n",
      "I know a pretty little place in Southern California, down San Diego way. (4110.757376145682, -124.82043850421906)\n",
      "This that is noon but yes apple whatever did regression variable (9968.166820575389, -101.2786717414856)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In a fixed-effects model only time-varying variables can be used.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"I know a pretty little place in Southern California, down San Diego way.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"This that is noon but yes apple whatever did regression variable\"\n",
    "print(sentence, perplexity(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22f750",
   "metadata": {},
   "source": [
    "### Define a validation set, such as 1,000 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "236b7f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_valid (100, 7)\n",
      "   post_id  parent_id  comment_id  \\\n",
      "0       75        NaN         NaN   \n",
      "1   112493        NaN         NaN   \n",
      "\n",
      "                                                text category  \\\n",
      "0  Where can I find useful R tutorials with vario...    title   \n",
      "1  How does the inclusion of an intercept change ...    title   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  where can i find useful r tutorials with vario...        11  \n",
      "1  how does the inclusion of an intercept change ...        14  \n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "df_valid = df_full[(df_full.category == 'title') & (df_full.n_tokens > 10)].sample(100, random_state = 88).reset_index(drop = True)\n",
    "print(\"df_valid\",df_valid.shape)\n",
    "print(df_valid.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f433907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus):\n",
    "    # start by calculating the total number of tokens in the corpus\n",
    "    all_sentences = ' '.join(corpus)\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    N = len(all_tokens)\n",
    "    logproba = 0\n",
    "    perps = []\n",
    "    for sentence in corpus:\n",
    "        pp, logp = perplexity(sentence)\n",
    "        logproba += logp\n",
    "        perps.append(pp)\n",
    "        print (\"{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{}\".format(pp, np.mean(perps), logp, logproba, np.exp( - logproba / (N  )), sentence  ))\n",
    "\n",
    "    return np.exp( - logproba / (N)), perps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1b2c6",
   "metadata": {},
   "source": [
    "### Transform that validation set into sequences of tokens using the training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fa5b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4318.05\t4318.05\t-92.08\t-92.08\t1.06\twhere can i find useful r tutorials with various implementations ?\n",
      "283.72\t2300.88\t-79.07\t-171.15\t1.12\thow does the inclusion of an intercept change the variability of the residual ?\n",
      "4255.90\t2952.56\t-133.70\t-304.84\t1.22\tpercentages of variance in y explained by x ,.. x n - adds up to ?\n",
      "8492.59\t4337.57\t-99.52\t-404.36\t1.31\twhen kl divergence and ks test will show inconsistent results ?\n",
      "175.92\t3505.24\t-56.87\t-461.23\t1.36\tsignificance test of a coefficient of a log - log regression\n",
      "430.51\t2992.78\t-78.84\t-540.08\t1.43\ta fisher information metric which doesn ' t refer to any exponential family\n",
      "992.76\t2707.06\t-82.81\t-622.88\t1.51\tare there realistic relevant use - cases for one way anova ?\n",
      "42.52\t2374.00\t-41.25\t-664.13\t1.55\twhat type of regression model do i need to use ?\n",
      "2931.84\t2435.98\t-127.73\t-791.87\t1.69\tcomparing classical and robust huber - white sandwich heteroscedasticity consistent standard errors in linear multiple regression\n",
      "587.64\t2251.14\t-89.27\t-881.13\t1.80\tsatterthwaite degrees of freedom in a mixed model change drastically depending on the dv\n",
      "87.39\t2054.44\t-67.06\t-948.19\t1.88\twhy is the proportion of total variance of the data explained by the model ?\n",
      "1637.89\t2019.73\t-81.41\t-1029.60\t1.98\thow to assess linearity and equal variance assumptions for regression modeling\n",
      "870.35\t1931.31\t-128.61\t-1158.21\t2.16\twhy doesn ' t using an interaction effect modification term give the same results as subsetting the data ?\n",
      "48.62\t1796.84\t-85.45\t-1243.66\t2.28\tis it okay to fit a linear model to my data , and if so , are the errors normally distributed ?\n",
      "55.69\t1680.76\t-68.34\t-1312.00\t2.39\thow can a t - test be used to compare the distributions between groups of data ?\n",
      "1320.37\t1658.23\t-100.60\t-1412.60\t2.55\tin benchmark tests should i use formulas assuming a population or a sample ?\n",
      "803.13\t1607.93\t-80.26\t-1492.86\t2.69\tmultivariate regression and use of proportion type variables as dv in it\n",
      "1251.83\t1588.15\t-164.04\t-1656.90\t3.00\tdoes r use tukey or tukey - kramer test corrected for unequal sample size and does it use the multivariate t distribution ?\n",
      "114.15\t1510.57\t-104.23\t-1761.13\t3.22\thow to create a random variables in a simulation using skewness and kurtosis as well as average and standard deviation input ?\n",
      "1014.73\t1485.78\t-103.84\t-1864.96\t3.45\tforecasting with use of pca variables as independent and one ternary dependent variable in r\n",
      "625.90\t1444.83\t-70.83\t-1935.79\t3.62\tcan cox models be used with time - varying covariates ?\n",
      "464.21\t1400.26\t-67.54\t-2003.34\t3.78\ttesting to compare for impact of independent variable on dependent variable\n",
      "66.93\t1342.29\t-63.06\t-2066.39\t3.94\twhat is the best way to do a seasonal arma or arima in python ?\n",
      "386.43\t1302.46\t-77.44\t-2143.83\t4.15\trandom intercepts as response variables is there a name for this method ?\n",
      "33529.60\t2591.55\t-125.04\t-2268.88\t4.51\tmerging a few separately trained skikit - learn mlpregressor models into one\n",
      "759.86\t2521.10\t-86.23\t-2355.11\t4.78\tdifference between two separate multiple regression analyses and one combined using dummy variables\n",
      "284.98\t2438.28\t-67.83\t-2422.94\t5.00\thow do you show that a markov chain has not mixed ?\n",
      "8109.71\t2640.83\t-108.01\t-2530.95\t5.37\tproof of direct proportionality between hazard rate function and probability density function\n",
      "481.21\t2566.36\t-92.64\t-2623.59\t5.71\thow to incorporate uncertainty and noise information in training and prediction of neural networks ?\n",
      "382.53\t2493.57\t-83.26\t-2706.85\t6.03\thow to test significance of difference between regression coefficients for multiple interaction categories ?\n",
      "167.10\t2418.52\t-97.25\t-2804.10\t6.44\tis there a way to find the specific number of predictors necessary for my caret random forests model ?\n",
      "2488.33\t2420.70\t-117.29\t-2921.39\t6.96\ti need help with choosing a mid - long term forecastic method for this demand\n",
      "160.36\t2352.21\t-96.47\t-3017.86\t7.42\tgetting nan value for x - squared and n a for p - value in chi - square test\n",
      "513.71\t2298.13\t-99.87\t-3117.73\t7.93\tmann - whitney u test or kruskal wallis test for comparing median of two groups ?\n",
      "245.42\t2239.48\t-88.05\t-3205.77\t8.40\thow do i conduct a comparison of two x frequency tables that hold different data ?\n",
      "327.58\t2186.37\t-156.38\t-3362.15\t9.32\tis the assumption of indepence only for the sampled values informing the regression , or should it also apply to the cells of a prediction grid ?\n",
      "385.18\t2137.69\t-65.49\t-3427.64\t9.74\twhat is the value of π in this experiment bernoulli ?\n",
      "982.82\t2107.30\t-75.79\t-3503.44\t10.24\tfind general correlation between function with itself and other input data\n",
      "86.83\t2055.49\t-58.03\t-3561.47\t10.64\tnon - parametric test if two samples are drawn from the same distribution\n",
      "6288.44\t2161.32\t-113.70\t-3675.17\t11.48\tquestion about idempotent matrix how can m i - m equal zero ?\n",
      "493.11\t2120.63\t-93.01\t-3768.18\t12.21\twhy can certain variables in a multiple regression not be included in logarithmic form ?\n",
      "250.05\t2076.09\t-99.39\t-3867.57\t13.04\tis there a rank correlation coefficient that takes into account the presence of different elements in two rankings\n",
      "810.40\t2046.66\t-107.16\t-3974.73\t14.00\thow do i run a two - part model of health care expenditures in stata ?\n",
      "103.72\t2002.50\t-60.34\t-4035.08\t14.58\thow to interpret two types of measurement scale in a single questionnaire ?\n",
      "1137.07\t1983.27\t-91.47\t-4126.55\t15.49\twhen should you check if assumptions are met when using stepwise selection ?\n",
      "317.36\t1947.05\t-63.36\t-4189.91\t16.15\tdo classification trees need to consider the correlation between attributes ?\n",
      "1000.52\t1926.91\t-96.72\t-4286.62\t17.23\twhich adjusted p value is the most accurate one fdr , none , bonferroni\n",
      "403.52\t1895.18\t-66.00\t-4352.63\t18.00\twhat is the expected dot product of two evolving vectors ?\n",
      "83.13\t1858.20\t-97.25\t-4449.88\t19.20\tif the value of in linear regression analysis is . , is it good or bad ? what are the alternatives ?\n",
      "2386.02\t1868.75\t-124.44\t-4574.31\t20.85\tis downsampling okay for logistic regression if i only care about relative ordering roc auc ?\n",
      "80.05\t1833.68\t-100.80\t-4675.12\t22.29\tis the average of a sum equal to the average of the sum of the averages of parts of the original sum ?\n",
      "97.57\t1800.29\t-54.97\t-4730.08\t23.12\thow to find the expected distance between two uniformly distributed points ?\n",
      "2584.16\t1815.08\t-110.00\t-4840.08\t24.88\tsampling technique to estimate how many toxic waste sites are in a country ?\n",
      "439.40\t1789.61\t-97.37\t-4937.45\t26.54\tdoes it make sense to log - transform the dependent when using gradient boosted trees ?\n",
      "2521.18\t1802.91\t-125.32\t-5062.77\t28.84\tinterpreting r output from exploratory factor analysis regarding rejection of null hypothesis of goodness of fit\n",
      "232.34\t1774.86\t-81.72\t-5144.49\t30.45\thow to specify a mixed model in lmer for between - subjects longitudinal study ?\n",
      "661.53\t1755.33\t-77.93\t-5222.43\t32.06\thow to calculate mean sample - based reliability estimates used for analyses\n",
      "495.43\t1733.61\t-86.88\t-5309.30\t33.97\thow to select cut - point for making classifications table for logistic regression ?\n",
      "242.54\t1708.34\t-131.79\t-5441.09\t37.07\twhen using repeated - measures anova in r , what does it mean to specify error subject instead of error subject a b ?\n",
      "5449.45\t1770.69\t-206.48\t-5647.57\t42.52\tanova result indicated that there were sig . differences while tukey post - hoc test revealed that there were no statistically sig . differences\n",
      "481.45\t1749.55\t-80.30\t-5727.87\t44.85\thow the de finetti ' s representation theorem works in this case ?\n",
      "2307.42\t1758.55\t-108.41\t-5836.28\t48.20\tmethod s to predict binary outcome from onset of variation in time series data\n",
      "391.95\t1736.86\t-83.60\t-5919.88\t50.95\tlogistic regression with independent categorical variables with more than two possible values using stata\n",
      "883.05\t1723.52\t-74.62\t-5994.50\t53.54\tdistribution of the y - coordinate of a d poisson process\n",
      "607.50\t1706.35\t-102.55\t-6097.05\t57.31\thow does the log p x , y normalize the point - wise mutual information ?\n",
      "189.90\t1683.37\t-73.45\t-6170.50\t60.18\twhat is the probability of success when two independent predictors co - occur ?\n",
      "187.92\t1661.05\t-89.01\t-6259.51\t63.84\tdetermining how much data is needed for statistical significance of a percentage difference between two points ?\n",
      "4275.27\t1699.50\t-108.69\t-6368.20\t68.62\tpermutation of input features of svm , simple logistic and knn classifiers ?\n",
      "754.34\t1685.80\t-79.51\t-6447.71\t72.34\ttest whether difference in proportions differs from a non - zero constant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959.56\t1675.42\t-82.40\t-6530.10\t76.41\twhy can ' t ridge regression provide better interpretability than lasso ?\n",
      "1368.39\t1671.10\t-101.10\t-6631.20\t81.71\twhat to do when cfa fit for multi - item scale is bad ?\n",
      "1056.39\t1662.56\t-90.51\t-6721.72\t86.77\tback - of - the - envelope calculation in a triple - differences\n",
      "1955.64\t1666.58\t-113.68\t-6835.39\t93.58\tr lmer , time point longitudinal data , non linear , messy residual help !\n",
      "1439.05\t1663.50\t-87.26\t-6922.66\t99.16\tjohansen ' s cointegration test in small sample under non - normality\n",
      "1156.23\t1656.74\t-126.95\t-7049.61\t107.88\tare predicted probabilities different when using a logistic gee model vs . a logistic random effects model ?\n",
      "411.69\t1640.36\t-84.28\t-7133.89\t114.09\thow do i decide what span to use in loess regression in r ?\n",
      "7821.69\t1720.63\t-98.61\t-7232.50\t121.81\tdistribution of the th percentile of a geometric brownian motion simulation\n",
      "2162.40\t1726.30\t-84.47\t-7316.97\t128.84\tsome lectures examples on q - learning using feature linear approximation\n",
      "450.55\t1710.15\t-67.22\t-7384.19\t134.72\tdistribution of a second degree polynomial of a gaussian random variable\n",
      "2278.58\t1717.25\t-139.16\t-7523.35\t147.76\thow do we separate marginals from dependence using copulas , and why do we assume uniform marginals ?\n",
      "148.69\t1697.89\t-90.03\t-7613.38\t156.86\twhat model to use if factors are assumed to have a multiplicative effect on the dependent variable ?\n",
      "682.57\t1685.51\t-156.62\t-7770.00\t174.05\twhat is the probability that a girl is taller than a boy , with boys n , . and girls n , . ?\n",
      "227.41\t1667.94\t-59.69\t-7829.70\t181.09\tinterpreting , f - statistic p - value of a model\n",
      "216.20\t1650.66\t-102.15\t-7931.85\t193.80\tlearner in r to predict data from a complex function if a b then a b else a b\n",
      "334.41\t1635.17\t-63.94\t-7995.78\t202.21\tfinding a random variable on the sample space with given cdf\n",
      "794.06\t1625.39\t-93.48\t-8089.26\t215.15\tthe special case of the negative binomial , the geometric and calculation with scipy\n",
      "240.78\t1609.48\t-109.68\t-8198.94\t231.41\thow to estimate the probability of a rare event about which observations can only be made in quantized time ?\n",
      "1821.39\t1611.88\t-105.10\t-8304.04\t248.14\twhy may results from model with interaction term and stratified model be different ?\n",
      "1663.74\t1612.47\t-126.09\t-8430.13\t269.80\twhat ' s the practical difference between the johansen vs engle - granger tests for cointegration ?\n",
      "27741.48\t1902.79\t-133.00\t-8563.13\t294.72\twhy do rnns have a tendency to suffer from vanishing exploding gradient ?\n",
      "158.64\t1883.62\t-70.93\t-8634.06\t308.93\thow can i reject or confirm the null hypothesis without p - value ?\n",
      "6905.69\t1938.21\t-97.24\t-8731.30\t329.53\testimating a variable from its cosine corrupted by additive gaussian noise\n",
      "8586.13\t2009.69\t-99.64\t-8830.94\t352.07\twhy does pglm give different results thans log - plm ?\n",
      "114.19\t1989.53\t-94.76\t-8925.70\t374.94\twhy is the every data item in a sample is considered to be produced by a different random variable ?\n",
      "943.75\t1978.52\t-82.20\t-9007.90\t395.97\thow neural networks ' prediction in r works on periodic data ?\n",
      "717.30\t1965.38\t-105.21\t-9113.10\t424.62\twhat classifier allows me to classify object , object , object , none of those ?\n",
      "4049.33\t1986.87\t-191.05\t-9304.15\t482.05\tin propensity score matching , what violations or implications may result from having fitted propensity scores that are not centered at . ?\n",
      "79.04\t1967.40\t-83.03\t-9387.18\t509.38\thow do i interpret the very different results given by a logistic regression on a subset of variables ?\n",
      "49.81\t1948.03\t-54.71\t-9441.89\t528.22\tthe way to evaluate the importance of an independent variable in a regression model\n",
      "2021.24\t1948.76\t-197.90\t-9639.79\t602.40\tchecking ph assumption for a time dependent t cov variable in a cox ph - regression with or without the original covariate in the regression ?\n",
      " Corpus perplexity: 602.40\n"
     ]
    }
   ],
   "source": [
    "# Calculate Perplexity score on the validation set\n",
    "corpus = df_valid.tokens.values\n",
    "perplexity_score, scores = corpus_perplexity(corpus)\n",
    "print(\" Corpus perplexity: {:.2f}\".format(perplexity_score ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e218598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9740bc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"perplexity scores of sentences - histogram\")\n",
    "plt.hist([sc for sc in scores if sc < 5000], bins=30);\n",
    "plt.xlabel('perplexity')\n",
    "plt.ylabel('# of sentences')\n",
    "plt.grid(alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96309ca",
   "metadata": {},
   "source": [
    "### Tune the neural net and the parameters of the preprocessing phase to improve the model’s perplexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ccf260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
