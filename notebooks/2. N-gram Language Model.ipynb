{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9f4206",
   "metadata": {},
   "source": [
    "# M1 Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f05c2",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this preliminary milestone is to load and preprocess the dataset. The raw text is noisy and we want to remove nonwords and non-ASCII characters, keep punctuation to a minimum, and reduce the overall vocabulary of the corpus.\n",
    "\n",
    "- Although this corpus is not as noisy as a text directly extracted from a social network (for example, Twitter or Facebook), it is still not as structured as academic papers or newspaper articles. Furthermore, the corpus displays some interesting particularities, such as the presence of HTML markup and LaTeX-formatted equations. The corpus is also rich in specific entities, names of theorems, and statistical test algorithms, and it mixes colloquial writing with more formally structured paragraphs.\n",
    "\n",
    "\n",
    "- The garbage-in, garbage-out golden rule of machine learning is also applicable to language models. Simply put, if we skip the preprocessing/cleaning part of the project, the vocabulary of our language model will be too vast and noisy to make any sense. Generated text, for instance, may mix in mathematical symbols with punctuation signs or random HTML tags and numbers. By reducing the volume of the corpus vocabulary, we increase the relevance and quality of the generated text and improve the reliability of sentence selection based on their respective probabilities. We also reduce the memory imprint of our code and its execution time.\n",
    "\n",
    "\n",
    "- Preprocessing the text to reduce noise and vocabulary size is an iterative process. You should start simple and further refine the preprocessing steps after building and evaluating your first language models.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "553d6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad111b6f",
   "metadata": {},
   "source": [
    "## Load the dataset into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2b0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/data/stackexchange_812k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd7ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 812132 entries, 0 to 812131\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   post_id     812132 non-null  int64  \n",
      " 1   parent_id   75535 non-null   float64\n",
      " 2   comment_id  553076 non-null  float64\n",
      " 3   text        812132 non-null  object \n",
      " 4   category    812132 non-null  object \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 31.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bcadf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95af6951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498456    S. Haykin, Adaptive Filter Theory, 5th Edition...\n",
       "89243     Understanding the violation of the independenc...\n",
       "15152     Confusion over lmer and p-values: how do p-val...\n",
       "15904     Is a large control sample better than a balanc...\n",
       "170280    <p>Figure 1 there clarifies things a bit. All ...\n",
       "649371    Fair enough, I agree/stand corrected. I still ...\n",
       "112754    <p>There are some angles on this to consider. ...\n",
       "810641                  Any question @RiturajSinghRathore ?\n",
       "120845    <p>Your example is a very good one because it ...\n",
       "66418     Question about notation of expectation operato...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b201a",
   "metadata": {},
   "source": [
    "## Use regular expressions to remove elements that are not words, such as HTML tags, LaTeX expressions, URLs, digits, and line returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981ebe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML = \"<[^>]*>\"\n",
    "LATEX = \"\\$[^>]*\\$\"\n",
    "URLS = \"http\\S+\"\n",
    "CRS = \"[\\r\\n]+\"\n",
    "DIGITS = \"\\$[^>]*\\$\"\n",
    "SPACES = \"\\s\\s+\"\n",
    "PUNCT = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(PUNCT)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = re.sub(HTML,' ', text)\n",
    "    text = re.sub(LATEX,' ', text)\n",
    "    text = re.sub(URLS,' ', text)\n",
    "    text = re.sub(CRS,' ', text)\n",
    "    text = re.sub(DIGITS,' ', text)\n",
    "    text = re.sub(pattern,' ', text)\n",
    "    text = re.sub(SPACES,' ', text)\n",
    "    text = re.sub(DIGITS,' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2cac22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formulate hypotheses when'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('Formulate hypotheses when $\\mu_A < \\mu_B$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece878bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See my response to a href'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('See my response to <a href=\"https://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cae6a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "May fit better under: http://stats.stackexchange.com/questions/1906/data-mining-conferences?\n",
      "--------------------\n",
      "$X$ is the mean of something and is distributed as $Exp(1)$ and Y is the actual value (i.e. not the mean) with parameter $X = x$ such that $Y | X = x$ is distributed as $Pois(X = x)$. I'll give my own answer right now and hopefully, you can confirm it.\n",
      "--------------------\n",
      "What result do you get if you just use a random forest regression model instead of the classifier and then regressor?\n"
     ]
    }
   ],
   "source": [
    "# Sample of comments\n",
    "for p in df[df.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4d4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6302af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Glen b and samooch and Marius The formula expansion of factor symbol factor time will include main effects for both symbol and time. Furthermore, the 0 will only change the labeling of the effects. Instead of an Intercept term you will see the estimates for interaction of the lowest levels for symbol and time. If you wanted to avoid estimating a main effect for time you would need to use factor symbol factor time\n",
      "--------------------\n",
      "I don't understand your question. Are you asking what does it mean if all variables are most strongly correlated with PC1?\n",
      "--------------------\n",
      "Yes, I didn't know the term eigenface but that's what they are when I have just one output neuron with 6 output neurons I still get faces but they are much more noisy .\n"
     ]
    }
   ],
   "source": [
    "# Post clean sample of comments\n",
    "for p in df[df.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970bdf9",
   "metadata": {},
   "source": [
    "## Remove texts that contain blanks only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936605ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "435cbb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1422"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text.str.len() == 0].text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319346b",
   "metadata": {},
   "source": [
    "1422 out of 812132 entries have a zero length text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d07c3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.text.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ba2c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810710"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace21d07",
   "metadata": {},
   "source": [
    "## Remove texts that are extremely large or too short to add any information to the model. \n",
    "\n",
    "We want to keep paragraphs that contain at least a few words and remove the paragraphs that are composed of large numerical tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fbdfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "df['tokens'] = df.text.apply(lambda t : tokenizer.tokenize(t.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f036c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_tokens'] = df.tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b7e664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    810710.000000\n",
       "mean         63.246199\n",
       "std         122.586727\n",
       "min           1.000000\n",
       "25%          16.000000\n",
       "50%          36.000000\n",
       "75%          72.000000\n",
       "max       14835.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f038227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14835"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_tokens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b543f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791172, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df.n_tokens > 4) & (df.n_tokens < 5000)].reset_index(drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3274d6a",
   "metadata": {},
   "source": [
    "## Use a tokenizer to create a version of the original text that is a string of space-separated lowercase tokens. \n",
    "\n",
    "For instance,\n",
    "\n",
    "- Thank you!, This equation y = ax + by=ax+b, is very helpful.\n",
    "\n",
    "    would be transformed to:\n",
    "\n",
    "    thank you ! this equation , is very helpful .\n",
    "\n",
    "- “retrieve a distance matrix” is a matter of coding. It also might be irrelevant: one can imagine creative answers.\n",
    "\n",
    "    becomes, if you choose to remove double quotes from the original text:\n",
    "\n",
    "    retrieve a distance matrix is a matter of coding. it also might be irrelevant : one can imagine creative answers .\n",
    "\n",
    "Note that punctuation signs (, . : !) are also represented as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6194550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45cc342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_separated_lower(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return \" \".join(list(filter(lambda x: x not in ['“', \"”\"], tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00ff51be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieve a distance matrix is a matter of coding . it also might be irrelevant : one can imagine creative answers .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '“retrieve a distance matrix” is a matter of coding. It also might be irrelevant: one can imagine creative answers.'\n",
    "space_separated_lower(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57654c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.text.apply(space_separated_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4afec4",
   "metadata": {},
   "source": [
    "## Export the resulting DataFrame into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3df59f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "df.to_csv(\"../data/stackexchange_cleaned.csv\", quoting = csv.QUOTE_ALL, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5e15",
   "metadata": {},
   "source": [
    "# M2 N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d8539",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this second milestone of the liveProject, the objective is to build an n-gram language model that is defined by the probabilities of all the n-grams in the corpus. Assuming that each token only depends on n-1 previous tokens, the language model is fully defined by the probability of any token in the corpus given its n-1 previous tokens (the prefix).\n",
    "\n",
    "You will use the language model to complete the following tasks:\n",
    "\n",
    "Generate text and complete queries from sequences of n-grams, using temperature sampling to tune the randomness of the generated text.\n",
    "Calculate the probability of a sentence and select the most probable sentence among several candidates.\n",
    "Score the quality of the language model using perplexity.\n",
    "Handle out-of-vocabulary (OOV) tokens with Laplace smoothing.\n",
    "\n",
    "Although simple in its approach, an n-gram language model with additive smoothing is a fast and reliable way to build a model that you can exploit for simple tasks such as query completion and sentence selection, provided that the training dataset is large and specific enough for the domain of interest.\n",
    "\n",
    "This first language model will serve as a baseline for the more complex language models that we will create in subsequent tasks. It also underlines the different problems and challenges inherent to any NLP task, such as handling out-of-vocabulary tokens, the importance of cleaning the original raw data, and the quality assessment of a language model.\n",
    "\n",
    "An n-gram model is defined as the probabilities of all the n-grams in the corpus. Under certain Markovian independence assumptions, this is equivalent to evaluating the probability of any token given its n-1 previous tokens (the prefix). For a given prefix, the probabilities of all the following tokens add up to 1 and constitute the probability distribution of the prefix.\n",
    "\n",
    "For instance, in our current corpus, the prefix “how many” may be followed by the words “people,” “times,” or “ways,” with respective frequencies of 0.46, 0.31, and 0.23, while the prefix “the model” is followed by the words “parameters” or “is” or a period, with frequencies 0.43, 0.36, and 0.21, and so forth.\n",
    "\n",
    "In an n-gram language model, the probability of a token given a prefix of n-1 tokens is given by its maximum likelihood estimate (MLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544792aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some global parameters\n",
    "\n",
    "# Displaying all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# We will work with trigrams \n",
    "ngrams_degree = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e507a",
   "metadata": {},
   "source": [
    "## Split the dataset into a training and a testing subset. \n",
    "\n",
    "Use the category “title” for the testing set and the categories “comment” and “post” for the training set. The short length of titles will make them good candidates later as seeds for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59618ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/stackexchange_cleaned.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d45b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791172, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dd0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 791172 entries, 0 to 791171\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   post_id     791172 non-null  int64  \n",
      " 1   parent_id   74519 non-null   float64\n",
      " 2   comment_id  542016 non-null  float64\n",
      " 3   text        791172 non-null  object \n",
      " 4   category    791172 non-null  object \n",
      " 5   tokens      791172 non-null  object \n",
      " 6   n_tokens    791172 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 42.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bc746d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I am working on a machine control project. We ...\n",
       "1    I have a set of sea surface temperature SST mo...\n",
       "2    This is assuming , which does not seem realist...\n",
       "3    Adolphe Quetelet for his work on the average m...\n",
       "4    Transformation of X is for linear relation bet...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4055ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda t : tokenizer.tokenize(' '.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a208d728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['iagocarvalho', 'i', 'am', 'not', 'sure', '.', 'i', 'don', \"'\", 't', 'think', 'there', 'is', 'any', '...']),\n",
       "       list(['whuber', ',', 'yes', ',', 'that', 'too']),\n",
       "       list(['i', 'don', \"'\", 't', 'see', 'the', 'commands', 'you', 'used', 'to', 'scale', 'the', 'data', '.', 'perhaps', 'there', 'is', 'an', 'error', 'there', '.', 'if', 'you', 'plot', 'each', 'scaled', 'variable', 'versus', 'its', 'corresponding', 'raw', 'variable', ',', 'you', 'should', 'get', 'nothing', 'but', 'straight', 'lines', '.', '...', 'oh', ',', 'and', 'if', 'you', 'scaled', \"'\", 'result', \"'\", 'too', ',', 'it', \"'\", 's', 'no', 'wonder', ',', 'because', 'you', \"'\", 're', 'no', 'long', 'modeling', 'the', 'sme', 'response', 'variable', '.', 'either', 'way', ',', 'we', 'need', 'more', 'details', 'on', 'what', 'you', 'mean', 'by', 'scaling', '.']),\n",
       "       list(['user112758', 'could', 'you', 'post', 'the', 'equations', 'in', 'question', '?', 'i', \"'\", 'm', 'afraid', 'i', 'don', \"'\", 't', 'have', 'the', 'book']),\n",
       "       list(['in', 'my', 'experience', 'kde', 'is', 'good', 'for', 'finding', 'critical', 'points', 'maxes', ',', 'mins', 'and', 'inflection', 'points', '.', 'you', 'can', 'get', 'those', 'on', 'one', 'pass', '--', 'but', 'only', 'after', 'you', \"'\", 've', 'come', 'up', 'with', 'a', 'suitable', 'bandwidth', '.', 'the', 'density', 'profile', 'itself', 'from', 'basic', 'kde', 'has', 'bias', 'proportional', 'to', 'the', 'second', 'curvature', 'of', 'the', 'density', 'profile', ',', 'so', 'bias', 'correction', 'involves', 'using', 'kde', 'multiple', 'times', 'to', 'estimate', 'derivatives', 'of', 'densities', '.', 'once', 'you', 'go', 'down', 'the', 'kde', 'hole', ',', 'expect', 'to', 'be', 'there', 'for', 'a', 'while', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f920ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and testing subset\n",
    "df_train = df[df.category.isin(['post', 'comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e3cb316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training set: (707373, 7)\n",
      "\n",
      "   post_id  parent_id  comment_id  \\\n",
      "0    16054        NaN         NaN   \n",
      "1     9739        NaN         NaN   \n",
      "2   313248        NaN    762205.0   \n",
      "3     5596     5115.0         NaN   \n",
      "4   372739        NaN    700543.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  I am working on a machine control project. We ...     post   \n",
      "1  I have a set of sea surface temperature SST mo...     post   \n",
      "2  This is assuming , which does not seem realist...  comment   \n",
      "3  Adolphe Quetelet for his work on the average m...     post   \n",
      "4  Transformation of X is for linear relation bet...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  [i, am, working, on, a, machine, control, proj...       147  \n",
      "1  [i, have, a, set, of, sea, surface, temperatur...       253  \n",
      "2  [this, is, assuming, ,, which, does, not, seem...        12  \n",
      "3  [adolphe, quetelet, for, his, work, on, the, a...        38  \n",
      "4  [transformation, of, x, is, for, linear, relat...        37  \n",
      "\n",
      "-- Testing set (83799, 7)\n",
      "\n",
      "    post_id  parent_id  comment_id  \\\n",
      "14   252788        NaN         NaN   \n",
      "15    55807        NaN         NaN   \n",
      "20    45309        NaN         NaN   \n",
      "22   222746        NaN         NaN   \n",
      "36    16865        NaN         NaN   \n",
      "\n",
      "                                                 text category  \\\n",
      "14  Hypothesis testing t test for mean difference ...    title   \n",
      "15             Multi-armed bandit algorithms in Java?    title   \n",
      "20    Correlation between one variable and few others    title   \n",
      "22  Is it good practice to adjust for multiple com...    title   \n",
      "36  Notation conventions for random variables and ...    title   \n",
      "\n",
      "                                               tokens  n_tokens  \n",
      "14  [hypothesis, testing, t, test, for, mean, diff...        11  \n",
      "15  [multi, -, armed, bandit, algorithms, in, java...         8  \n",
      "20  [correlation, between, one, variable, and, few...         7  \n",
      "22  [is, it, good, practice, to, adjust, for, mult...        16  \n",
      "36  [notation, conventions, for, random, variables...         8  \n"
     ]
    }
   ],
   "source": [
    "# Display the dimensions of the dataframe \n",
    "print(\"-- Training set: {}\\n\".format(df_train.shape))\n",
    "# and the 1st 5 lines\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n-- Testing set {}\\n\".format(df_test.shape))\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2b7b7",
   "metadata": {},
   "source": [
    "## Build the matrix of prefix—word frequencies.\n",
    "\n",
    "- Use the ngrams function from nltk.utils to generate all n-grams from the corpus.\n",
    "\n",
    "\n",
    "- Set the following: left_pad_symbol = \\<s> and right_pad_symbol = \\</s>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb0e0a",
   "metadata": {},
   "source": [
    "### Counting bigrams and following tokens\n",
    "\n",
    "We build a counts object defined as a defaultdict(Counter). \n",
    "\n",
    "Taking into account all trigrams (ngrams_degree = 3) that we break into prefix (bigrams) followed by single tokens. \n",
    "\n",
    "The counts object will have the bigrams as keys and for each key a Counter of all the potential tokens. \n",
    "\n",
    "For instance, if the corpus contains a 100 instances of \"*how many people*\" and a 120 instances of \"*how many times*\" we would get the following entry:\n",
    "\n",
    "    counts[('how', 'many')] = Counter('people': 100, 'times': 120, .... )\n",
    "\n",
    "Similarly if the corpus contains \"*the model is*\" 500 times and \"*the model parameters*\" 200 times, we end up with:\n",
    "\n",
    "    counts[('the', 'model')] = Counter('is': 500, 'parameters': 200, .... )\n",
    "\n",
    "To split the tokens into bigramns we use the [ntlk.ngrams](https://www.nltk.org/api/nltk.html#nltk.util.ngrams) function:\n",
    "\n",
    "\n",
    "    Return the ngrams generated from a sequence of items, as an iterator.\n",
    "    For example:\n",
    "\n",
    "    >>> from nltk.util import ngrams\n",
    "    >>> list(ngrams([1,2,3,4,5], 3))\n",
    "    [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
    "\n",
    "The next cell should take a couple of minutes.\n",
    "\n",
    "Note that we build the mode on the training subset df_train and leave the testing subset aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a46619c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "63cf7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(sentence, ngrams_degree=3):\n",
    "    return ngrams(\n",
    "        sentence, \n",
    "        n = ngrams_degree,  \n",
    "        pad_right = True, \n",
    "        pad_left = True, \n",
    "        left_pad_symbol = \"<s>\", \n",
    "        right_pad_symbol = \"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8e1e33f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', '<s>', 'the'),\n",
       " ('<s>', '<s>', 'the', 'difference'),\n",
       " ('<s>', 'the', 'difference', 'between'),\n",
       " ('the', 'difference', 'between', 'the'),\n",
       " ('difference', 'between', 'the', 'two'),\n",
       " ('between', 'the', 'two', 'approaches'),\n",
       " ('the', 'two', 'approaches', 'is'),\n",
       " ('two', 'approaches', 'is', 'discussed'),\n",
       " ('approaches', 'is', 'discussed', 'here'),\n",
       " ('is', 'discussed', 'here', '</s>')]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "list(n_grams(sentence.split(), 4 ) )[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f2928730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 705964/705964 [01:16<00:00, 9262.12it/s]\n"
     ]
    }
   ],
   "source": [
    "counts = defaultdict(Counter)\n",
    "for tokens in tqdm(df_train.tokens.values):\n",
    "    for ngram in n_grams(tokens):      \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        counts[prefix][token] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "98074d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 3332935 bigrams\n"
     ]
    }
   ],
   "source": [
    "print(\"we have {} bigrams\".format(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "88319b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chance', 'versus'): \tCounter({'actual': 1, 'probability': 1, 'systematic': 1, 'randomness': 1})\n",
      "('in', 'statmod'): \tCounter({'and': 1})\n",
      "('players', 'lurking'): \tCounter({'around': 1})\n",
      "('causational', 'research'): \tCounter({'.': 1})\n",
      "('based', 'ensemble'): \tCounter({'methods': 2, 'method': 1})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,counts[prefix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e290138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_count = [ len(v)   for k,v in counts.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50a023b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "plt.hist(tokens_count, bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5aa5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839506 bigrams_with_single_tokens\n",
      "511760 bigrams_with_two_tokens\n"
     ]
    }
   ],
   "source": [
    "bigrams_with_single_tokens = [ k   for k,v in counts.items() if len(v) == 1 ]\n",
    "bigrams_with_two_tokens = [ k   for k,v in counts.items() if len(v) == 2 ]\n",
    "\n",
    "print(\"{} bigrams_with_single_tokens\".format(len(bigrams_with_single_tokens)))\n",
    "print(\"{} bigrams_with_two_tokens\".format(len(bigrams_with_two_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "03fb343e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>'): 11564, ('of', 'the'): 10815}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dict = { k:len(v)   for k,v in counts.items() if len(v) > 10000 }\n",
    "tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf39d7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix= ('<s>', '<s>') \n",
      "tokens= [('i', 122361), ('the', 31612), ('thanks', 22184), ('you', 22096), ('this', 17342), ('it', 16576), ('if', 16295), ('what', 14034), ('thank', 13274), ('yes', 12196)]\n",
      "707373\n"
     ]
    }
   ],
   "source": [
    "for prefix, tokens in counts.items():\n",
    "    print(\"prefix=\", prefix, \"\\ntokens=\", tokens.most_common(10))\n",
    "    print(sum(counts[prefix].values()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23498b",
   "metadata": {},
   "source": [
    "###  token / prefix probabilities\n",
    "\n",
    "To obtain token / prefix probabilities using the Maximum Likelihood Estimator, we must simply normalize each (prefix - token) count by the total number of the prefix occurence. \n",
    "\n",
    "$$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}$$\n",
    "\n",
    "\n",
    "Keeping the same defaultdict(Counter) structure for the freq object, we should obtain something similar to \n",
    "\n",
    "\n",
    "    freq[('how', 'many')] = {'people': 0.14, 'times': 120, .... }\n",
    "\n",
    "with \n",
    "* p(people / how many) = c('how many people') / c('how many') \n",
    "* p(times / how many) = c('how many times') / c('how many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b7419177",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(dict)\n",
    "for prefix, tokens in counts.items():\n",
    "    total = sum(counts[prefix].values())\n",
    "    for token, count in tokens.items():\n",
    "        freq[prefix][token] = count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3bb2bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tests', 'performs'): \t{'equally': 0.3333333333333333, 'really': 0.3333333333333333, 'one': 0.3333333333333333}\n",
      "('variable', 'alongside'): \t{'several': 0.25, 'the': 0.25, 'an': 0.5}\n",
      "('philosophies', 'diverge'): \t{'many': 1.0}\n",
      "('determined', 'groups'): \t{'it': 1.0}\n",
      "('basic', 'patterns'): \t{'very': 0.5, 'don': 0.5}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prefix = random.choice(list(freq.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,freq[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5217dc",
   "metadata": {},
   "source": [
    "## Write a text generation function with the following features:\n",
    "\n",
    "- Takes a bigram as input and generates the next token\n",
    "\n",
    "\n",
    "- Iteratively slides the prefix over the generated text so that the new prefix includes the most recent token; generates the next token\n",
    "\n",
    "\n",
    "- To generate each next token, samples the list of words associated with the prefix using the probability distribution of the prefix\n",
    "\n",
    "\n",
    "- Stops the text generation when a certain number of words have been generated or the latest token is a \\</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "faeea9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, n_words = 40):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        probabilities = list(freq[prefix].values())\n",
    "        text += ' ' + np.random.choice(candidates, p = probabilities)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "07288d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'model')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple('the model'.split()[-ngrams_degree+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d1f0c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the model for example , library , but still successful in analyzing . for more information on the previous thread at cdf raised to a normal distribution function , so here it ' s also shiny </s>\n",
      "\n",
      "that distribution , type , or a different q -- not only x property of the seasonal ma coefficients . these values are distributed over time are statistically signficantly greater than one definition of causality ? </s>\n",
      "\n",
      "to determine the lines join at s0 which may be i 1 here ? pc1 or pc2 , pt3 pc3 , and produced something different values of both groups ? in a certain application of this function assumes the data ? </s>\n"
     ]
    }
   ],
   "source": [
    "text      = 'the model'\n",
    "print()\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'that distribution'\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'to determine'\n",
    "print(generate(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec1451",
   "metadata": {},
   "source": [
    "## Write a function that can estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n",
    "\n",
    "\n",
    "Split the sentence into trigrams and use the chain rule to calculate the probability of the sentence as a product of the bigrams—tokens probabilities.\n",
    "\n",
    "- Estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n",
    "\n",
    "- Similar to the above process calculate the candidates and initial_probabilites from freq dictionary\n",
    "\n",
    "- Here we will modify the initial_probabilites using temperature and normalizing it to generate random candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4ee17",
   "metadata": {},
   "source": [
    "### Temperature sampling\n",
    "\n",
    "As you may have noticed, for some bigrams, one particular token may be much more frequent than the others potential tokens. \n",
    "\n",
    "For instance:\n",
    "\n",
    "* ('building', 'machine'): \t{'learning': 0.875, 'classification': 0.125}\n",
    "\n",
    "when generating the next token based on the bigram \"*building machine*\", most of the times the word \"learning\" will be chosen instead of \"classification\".\n",
    "\n",
    "In order to compensate these imbalances and improve the chances of less frequent tokens to be chosen we can sample with temperature.\n",
    "\n",
    "In order to increase the randomness of the next token selection given a prefix, we can flatten the distribution using the temperature $$\\tau$$ to define a new probability distribution as such:\n",
    "\n",
    "$$f_{\\tau}(p_i) = \\frac{ p_i^{\\frac{1}{\\tau}} }{ \\sum_j p_j^{\\frac{1}{\\tau}} }$$\n",
    "\n",
    "See [this post](https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling) for a more in-depth explanation on temperature sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5113e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temp(text, temperature = 1, n_words = 30):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        initial_probabilities = list(freq[prefix].values())\n",
    "        denom   = sum([p ** temperature for p in initial_probabilities])\n",
    "        probabilities  = [p ** temperature / denom  for p in initial_probabilities]\n",
    "        text  += ' ' + np.random.choice(candidates, p = probabilities)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20da256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "the model those ? note to myself what you represent negative intra - layer is with any application in retrieval , a confusion which is suggestive , it implicitly confuses estimation with\n",
      "0.5\n",
      "the model conditioned on these topics monte carlo approach by pesaran et al were the output contains 7 binary questions . yes value 1 0 14098 . 1 plus a column real\n",
      "1\n",
      "the model predictor predictor information is relevant to the approach i am trying to help people answer no . i can find some good opinion from you . edit yes , the\n",
      "3\n",
      "the model is not the same as the number of observations . </s>\n",
      "10\n",
      "the model . </s>\n"
     ]
    }
   ],
   "source": [
    "text  = 'the model'\n",
    "# text  = 'to determine'\n",
    "# text  = 'not sure'\n",
    "\n",
    "for tau in [0.01, 0.5, 1, 3, 10]:\n",
    "    print(tau)\n",
    "    print(generate_temp(text, temperature = tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d617432",
   "metadata": {},
   "source": [
    "## Implement the perplexity scoring function for a given sentence and for the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac1194",
   "metadata": {},
   "source": [
    "Let's now implement a way to measure the quality of our model.\n",
    "\n",
    "The idea is to estimate the probability of a test sentence given our model. \n",
    "An uncommon sentence should be less probable than a common one.\n",
    "\n",
    "\n",
    "Notes : \n",
    "  1. At this point the sentence should exist in the corpus. Our model does not know yet how to handle out-of-vocabulary (OOV) bigrams, trigrams or tokens.\n",
    "  2. To avoid the problem of underflow caused by multiplying multiple very small floats, we work in the log space:\n",
    "\n",
    "So instead of calculating perplexity with (case ngrams_degree = 3):\n",
    " \n",
    "$$PP(w_{1},\\cdots, w_N) = ( \\prod_{i = 3}^{N} \\frac{1}{ p(w_i/ w_{i-2}w_{i-1} )} )^{\\frac{1}{N}}$$\n",
    "\n",
    "We compute\n",
    "\n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 3}^{N} \\log {p(w_i/ w_{i-2}w_{i-1}} } ) ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ec86e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def perplexity(sentence):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "\n",
    "    for ngram in n_grams(sentence): \n",
    "        try:\n",
    "            prefix = ngram[:ngrams_degree-1] \n",
    "            token = ngram[ngrams_degree-1]\n",
    "            logprob += np.log(freq[prefix][token])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.exp(- logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "74e5f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 22.95] the difference between the two approaches is discussed here\n",
      "\n",
      "[perplexity 38.30] this question really belongs on a different site\n",
      "\n",
      "[perplexity 73.33] The function may only be linear in the region where the points were taken\n"
     ]
    }
   ],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"The function may only be linear in the region where the points were taken\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eca363",
   "metadata": {},
   "source": [
    "## Implement additive Laplace smoothing to give a non-zero probability to missing prefix—token combinations when calculating perplexity.\n",
    "\n",
    "### Out of Vocabulary (OOV) \n",
    "\n",
    "The main weakness of our model so far is that it does not know how to handle elements that are not already in the original corpus.\n",
    "\n",
    "Since both when generating text and when calculating perplexity we use the count of the prefix in the corpus, when that prefix is missing, the counts = 0  which causes problems with logs and divisions.\n",
    "\n",
    "To remediate to that problem we can artificially assign a probability (although a very low one) to missing ngrams and tokens.\n",
    "\n",
    "This method is called Laplace smoothing. It relies on calculating the frequency of a token / prefix with:\n",
    "\n",
    "$$ p(token / prefix) = \\frac{ count( prefix + token) + \\delta}{count(prefix) + \\delta \\times |N| }$$\n",
    "\n",
    "\n",
    "Where \n",
    "\n",
    "* N is the total number of prefixes in the model\n",
    "* delta is an arbitrary number \n",
    "\n",
    "When the prefix is missing from the original corpus, the probability of a token / prefix will now be:\n",
    "\n",
    "$$p(token / prefix) = \\frac{1} { | N |}$$\n",
    "\n",
    "Let's implement that perplexity with Laplace Smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26d80385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity_laplace(sentence, delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    for ngram in n_grams(sentence): \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        if prefix in list(counts.keys()):\n",
    "            total = sum(counts[prefix].values())\n",
    "            if token in counts[prefix].keys():\n",
    "                logprob += np.log((counts[prefix][token] + delta)/ (total + delta * N))\n",
    "            else:\n",
    "                logprob += np.log((delta) / (total + delta * N ))\n",
    "        else:\n",
    "            logprob += - np.log(N)\n",
    "  \n",
    "    return np.exp(- logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7fc6597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 145.18] this model belongs on a different planet\n",
      "[perplexity 35.54] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity of sentences that were not present in the original corpus.\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bae00",
   "metadata": {},
   "source": [
    "## Calculate the perplexity of the language model on the test set composed of titles.\n",
    "\n",
    "Perplexity on the test corpus and sentence probability.\n",
    "\n",
    "How do we calculate the perplexity of a model on a test corpus?\n",
    "\n",
    "Let's say we have *m* sentences in the corpus, the perplexity of the corpus is given by \n",
    "\n",
    "$$ PP(Corpus) = P(S_1, \\cdots, S_m)^{-\\frac{1}{N}} $$\n",
    "\n",
    "We can assume that the sentences are independent\n",
    "\n",
    "$$ PP(Corpus) = (\\prod_{k = 1}^{m}  P(S_k))^{-\\frac{1}{N}} $$\n",
    "\n",
    "Which we calculate in the log space to avoid underflow\n",
    "\n",
    "$$ PP(Corpus) = \\exp ( -\\frac{1}{N} \\sum_{k = 1}^{m}  log(P(S_k)) $$\n",
    "\n",
    "So to calculate the perplexity on a test corpus we need to calculate the probability of each single sentence.\n",
    "\n",
    "The following function calculates the probability of a sentence. \n",
    "\n",
    "Instead of using laplace smoothing to deal with the missing bigrams and tokens, we will simply skip missing elements to make the function faster.\n",
    "Implementing laplace smoothing requires several extra conditions that are taking too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "be16262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_probability(sentence, delta = 1, ngrams_degree = 3):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    logprob = 0\n",
    "    for ngram in n_grams(sentence, ngrams_degree):\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        try:\n",
    "            logprob += np.log( freq[prefix][token] )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "14500436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus, ngrams_degree = 3):\n",
    "    # start by calculating the total number of tokens in the corpus\n",
    "    all_sentences = ' '.join(corpus)\n",
    "\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    N = len(tokens)\n",
    "\n",
    "    logprob = 0\n",
    "    probs = []\n",
    "    for sentence in tqdm(corpus):\n",
    "        lp = sentence_log_probability(sentence, ngrams_degree)\n",
    "        probs.append(lp)\n",
    "        if lp != np.inf:\n",
    "            logprob += lp\n",
    "        else:\n",
    "            print(lp)\n",
    "#     print(probs)        \n",
    "    print(logprob, N)\n",
    "    return np.exp( - logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e48133c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 10044.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31728.947433616246 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The perplexity of a sample of 1000 titles\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d104b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83799/83799 [00:02<00:00, 32080.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2671335.5473894435 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the perplexity of the whole test corpus\n",
    "corpus_perplexity(df_test.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22982ac3",
   "metadata": {},
   "source": [
    "## Try to improve the perplexity score of your model as follows:\n",
    "\n",
    "- Modify the preprocessing phase of the corpus.\n",
    "\n",
    "\n",
    "- Increase or decrease the number of tokens in the model (bigrams, 4-grams, and so on).\n",
    "\n",
    "\n",
    "- Vary the delta parameter in the additive Laplace smoothing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a4b4c",
   "metadata": {},
   "source": [
    "### Modify the preprocessing phase of the corpus.\n",
    "\n",
    "Not clear what should change, so not tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f1819",
   "metadata": {},
   "source": [
    "### Increase or decrease the number of tokens in the model (bigrams, 4-grams, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "35ea400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 19488.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-32098.052657143264 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The perplexity of a sample of 1000 titles and using 4-grams\n",
    "# original 3-gram score was -31728.947433616246\n",
    "# did perplexity increase?\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus, ngrams_degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "89fab091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83685/83685 [00:02<00:00, 32455.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2661949.885555473 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the perplexity of the whole test corpus using 4-grams\n",
    "# original 3-gram score was -2671335.5473894435\n",
    "# did perplexity decrease?\n",
    "corpus_perplexity(df_test.text.values, ngrams_degree=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7f5d6",
   "metadata": {},
   "source": [
    "### Vary the delta parameter in the additive Laplace smoothing step.\n",
    "\n",
    "Perplexity using delta value of 10:\n",
    "\n",
    "[perplexity 145.18] this model belongs on a different planet\n",
    "\n",
    "[perplexity 35.54] this question really belongs on a different site.\n",
    "\n",
    "Below using a delta value of 20 seems to reduce perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3f70660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 110.46] this model belongs on a different planet\n",
      "[perplexity 33.31] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "# calculate the perplexity of sentences that were not present in the original corpus.\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 20), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 20), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db77e5d",
   "metadata": {},
   "source": [
    "## Building an n-gram language model using NLTK\n",
    "\n",
    "Since version 3.4 the nltk library includes a language model module.\n",
    "\n",
    "Let's install the right version of nltk. Feel free to install any version > 3.4.5. \n",
    "\n",
    "After running the pip install command below you will need to restart the runtime. This will erase all the local variables. So we will reload and prepare the dataset from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "71763462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.5'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "151484ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "ngrams_degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "de375fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe, shuffle it and reset the index\n",
    "# ../data/stackexchange_812k.csv\n",
    "# ../data/stackexchange_cleaned.csv\n",
    "df = pd.read_csv('../data/stackexchange_812k.tokenized.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "36263207",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda txt : txt.split())\n",
    "df_train = df[df.category.isin(['post','comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ec7fb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(ngrams_degree, df_train.tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "73e61f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = MLE(ngrams_degree,vocabulary=Vocabulary(unk_cutoff = 20))\n",
    "\n",
    "# fit the model\n",
    "model.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ccd7b",
   "metadata": {},
   "source": [
    "Then you can use the perplexity and generate functions of the lm module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a11d1a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x7f3e3e4ce750>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(df_test.tokens.values[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8ef602ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.52360717347284"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perplexity(ngrams(df_test.tokens.values[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b372e7",
   "metadata": {},
   "source": [
    "### Signature: model.generate(num_words=1, text_seed=None, random_seed=None)\n",
    "\n",
    "Generate words from the model.\n",
    "\n",
    ":param int num_words: How many words to generate. By default 1.\n",
    "\n",
    ":param text_seed: Generation can be conditioned on preceding context.\n",
    "\n",
    ":param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
    "makes the random sampling part of generation reproducible.\n",
    "\n",
    ":return: One (str) word or a list of words generated from model.\n",
    "\n",
    "Examples:\n",
    "\n",
    "\\>>> from nltk.lm import MLE\n",
    "\n",
    "\\>>> lm = MLE(2)\n",
    "\n",
    "\\>>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
    "\n",
    "\\>>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
    "\n",
    "\\>>> lm.generate(random_seed=3)\n",
    "\n",
    "\\'a'\n",
    "\n",
    "\\>>> lm.generate(text_seed=['a'])\n",
    "\n",
    "\\'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "98697ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'university', '-', 'but', 'this', 'paper', 'it', 'doesn', \"'\", 't']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(num_words=10, random_seed=2, text_seed=['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1e32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
