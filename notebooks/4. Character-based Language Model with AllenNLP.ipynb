{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9f4206",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# M1 Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f05c2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this preliminary milestone is to load and preprocess the dataset. The raw text is noisy and we want to remove nonwords and non-ASCII characters, keep punctuation to a minimum, and reduce the overall vocabulary of the corpus.\n",
    "\n",
    "- Although this corpus is not as noisy as a text directly extracted from a social network (for example, Twitter or Facebook), it is still not as structured as academic papers or newspaper articles. Furthermore, the corpus displays some interesting particularities, such as the presence of HTML markup and LaTeX-formatted equations. The corpus is also rich in specific entities, names of theorems, and statistical test algorithms, and it mixes colloquial writing with more formally structured paragraphs.\n",
    "\n",
    "\n",
    "- The garbage-in, garbage-out golden rule of machine learning is also applicable to language models. Simply put, if we skip the preprocessing/cleaning part of the project, the vocabulary of our language model will be too vast and noisy to make any sense. Generated text, for instance, may mix in mathematical symbols with punctuation signs or random HTML tags and numbers. By reducing the volume of the corpus vocabulary, we increase the relevance and quality of the generated text and improve the reliability of sentence selection based on their respective probabilities. We also reduce the memory imprint of our code and its execution time.\n",
    "\n",
    "\n",
    "- Preprocessing the text to reduce noise and vocabulary size is an iterative process. You should start simple and further refine the preprocessing steps after building and evaluating your first language models.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553d6381",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad111b6f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load the dataset into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2b0dcc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/data/stackexchange_812k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd7ce3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 812132 entries, 0 to 812131\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   post_id     812132 non-null  int64  \n",
      " 1   parent_id   75535 non-null   float64\n",
      " 2   comment_id  553076 non-null  float64\n",
      " 3   text        812132 non-null  object \n",
      " 4   category    812132 non-null  object \n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 31.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bcadf5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eliciting priors from experts</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is normality?</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What are some valuable Statistical Analysis op...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Assessing the significance of differences in d...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0        1        NaN         NaN   \n",
       "1        2        NaN         NaN   \n",
       "2        3        NaN         NaN   \n",
       "3        4        NaN         NaN   \n",
       "4        6        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0                      Eliciting priors from experts    title  \n",
       "1                                 What is normality?    title  \n",
       "2  What are some valuable Statistical Analysis op...    title  \n",
       "3  Assessing the significance of differences in d...    title  \n",
       "4  The Two Cultures: statistics vs. machine learn...    title  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95af6951",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146574    <p>In this question on <a href=\"https://stacko...\n",
       "165751    <p>My company makes widgets. We take a random ...\n",
       "49517     Determining the PMF of the maximum of dependen...\n",
       "346266    Did you find out what different inputs (levels...\n",
       "809970    Is this is a balanced design? Also, I assume t...\n",
       "330062    @PeterFlom I've cleared the question up a bit ...\n",
       "566356    The model would be different if the categories...\n",
       "806572    @rvl Thanks for the interest. I asked this bec...\n",
       "96792     <p><strong>N.B.</strong>: <em>This was previou...\n",
       "15296     How to explain smoothing functions in the logi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b201a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Use regular expressions to remove elements that are not words, such as HTML tags, LaTeX expressions, URLs, digits, and line returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981ebe2a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "HTML = \"<[^>]*>\"\n",
    "LATEX = \"\\$[^>]*\\$\"\n",
    "URLS = \"http\\S+\"\n",
    "CRS = \"[\\r\\n]+\"\n",
    "DIGITS = \"\\$[^>]*\\$\"\n",
    "SPACES = \"\\s\\s+\"\n",
    "PUNCT = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(PUNCT)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = re.sub(HTML,' ', text)\n",
    "    text = re.sub(LATEX,' ', text)\n",
    "    text = re.sub(URLS,' ', text)\n",
    "    text = re.sub(CRS,' ', text)\n",
    "    text = re.sub(DIGITS,' ', text)\n",
    "    text = re.sub(pattern,' ', text)\n",
    "    text = re.sub(SPACES,' ', text)\n",
    "    text = re.sub(DIGITS,' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2cac22d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formulate hypotheses when'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('Formulate hypotheses when $\\mu_A < \\mu_B$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece878bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See my response to a href'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('See my response to <a href=\"https://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cae6a43",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "@ssdecontrol Yes, true. My comment was at least partly in jest. Should have put a smiley at the end. Sorry!\n",
      "--------------------\n",
      "I'm still carefully reading the references and will likely have follow up questions, but this is most definitely the answer I was looking for.\n",
      "--------------------\n",
      "Have a look at this question: http://stats.stackexchange.com/questions/77573/invariance-property-of-mle-what-is-the-mle-of-theta2-of-normal-barx2\n"
     ]
    }
   ],
   "source": [
    "# Sample of comments\n",
    "for p in df[df.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4d4341",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.text = df.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6302af1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Note that if you have more than 2 classes then 55 is far better then coin flipping. several classes sounds to be more than 2 classes\n",
      "--------------------\n",
      "winperikle very relevant indeed thanks! Though, i wonder how should I include the 3rd order interaction term. Any idea?\n",
      "--------------------\n",
      "could you make the statistical query clearer, the question is heavily weighted towards code which would be more suitable to stack overflow. Are you wanting readers to work out why code if giving an unexpected answer if so try SO or are you asking for the statistical explanation of the observed results? For the latter more figures and background to your data would be needed.\n"
     ]
    }
   ],
   "source": [
    "# Post clean sample of comments\n",
    "for p in df[df.category == 'comment'].text.sample(3).values:\n",
    "  print('-' * 20)\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a970bdf9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Remove texts that contain blanks only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936605ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "435cbb99",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1422"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text.str.len() == 0].text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319346b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1422 out of 812132 entries have a zero length text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d07c3cf3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df.text.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82ba2c26",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810710"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace21d07",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Remove texts that are extremely large or too short to add any information to the model. \n",
    "\n",
    "We want to keep paragraphs that contain at least a few words and remove the paragraphs that are composed of large numerical tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2fbdfea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "df['tokens'] = df.text.apply(lambda t : tokenizer.tokenize(t.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f036c7c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['n_tokens'] = df.tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b7e664",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    810710.000000\n",
       "mean         63.246199\n",
       "std         122.586727\n",
       "min           1.000000\n",
       "25%          16.000000\n",
       "50%          36.000000\n",
       "75%          72.000000\n",
       "max       14835.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f038227b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14835"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.n_tokens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b543f16",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791172, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(df.n_tokens > 4) & (df.n_tokens < 5000)].reset_index(drop = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3274d6a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Use a tokenizer to create a version of the original text that is a string of space-separated lowercase tokens. \n",
    "\n",
    "For instance,\n",
    "\n",
    "- Thank you!, This equation y = ax + by=ax+b, is very helpful.\n",
    "\n",
    "    would be transformed to:\n",
    "\n",
    "    thank you ! this equation , is very helpful .\n",
    "\n",
    "- “retrieve a distance matrix” is a matter of coding. It also might be irrelevant: one can imagine creative answers.\n",
    "\n",
    "    becomes, if you choose to remove double quotes from the original text:\n",
    "\n",
    "    retrieve a distance matrix is a matter of coding. it also might be irrelevant : one can imagine creative answers .\n",
    "\n",
    "Note that punctuation signs (, . : !) are also represented as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6194550",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45cc342c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def space_separated_lower(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return \" \".join(list(filter(lambda x: x not in ['“', \"”\"], tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00ff51be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retrieve a distance matrix is a matter of coding . it also might be irrelevant : one can imagine creative answers .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '“retrieve a distance matrix” is a matter of coding. It also might be irrelevant: one can imagine creative answers.'\n",
    "space_separated_lower(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57654c1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21234/3640618307.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_separated_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7-dev/envs/allennlp=2.8.0/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21234/4247980895.py\u001b[0m in \u001b[0;36mspace_separated_lower\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspace_separated_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'“'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"”\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7-dev/envs/allennlp=2.8.0/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/.pyenv/versions/3.7-dev/envs/allennlp=2.8.0/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/.pyenv/versions/3.7-dev/envs/allennlp=2.8.0/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPARENS_BRACKETS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Optionally convert parentheses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7-dev/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['tokens'] = df.text.apply(space_separated_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4afec4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Export the resulting DataFrame into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df59f49",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "df.to_csv(\"../data/stackexchange_cleaned.csv\", quoting = csv.QUOTE_ALL, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b5e15",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# M2 N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d8539",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Objective\n",
    "\n",
    "In this second milestone of the liveProject, the objective is to build an n-gram language model that is defined by the probabilities of all the n-grams in the corpus. Assuming that each token only depends on n-1 previous tokens, the language model is fully defined by the probability of any token in the corpus given its n-1 previous tokens (the prefix).\n",
    "\n",
    "You will use the language model to complete the following tasks:\n",
    "\n",
    "Generate text and complete queries from sequences of n-grams, using temperature sampling to tune the randomness of the generated text.\n",
    "Calculate the probability of a sentence and select the most probable sentence among several candidates.\n",
    "Score the quality of the language model using perplexity.\n",
    "Handle out-of-vocabulary (OOV) tokens with Laplace smoothing.\n",
    "\n",
    "Although simple in its approach, an n-gram language model with additive smoothing is a fast and reliable way to build a model that you can exploit for simple tasks such as query completion and sentence selection, provided that the training dataset is large and specific enough for the domain of interest.\n",
    "\n",
    "This first language model will serve as a baseline for the more complex language models that we will create in subsequent tasks. It also underlines the different problems and challenges inherent to any NLP task, such as handling out-of-vocabulary tokens, the importance of cleaning the original raw data, and the quality assessment of a language model.\n",
    "\n",
    "An n-gram model is defined as the probabilities of all the n-grams in the corpus. Under certain Markovian independence assumptions, this is equivalent to evaluating the probability of any token given its n-1 previous tokens (the prefix). For a given prefix, the probabilities of all the following tokens add up to 1 and constitute the probability distribution of the prefix.\n",
    "\n",
    "For instance, in our current corpus, the prefix “how many” may be followed by the words “people,” “times,” or “ways,” with respective frequencies of 0.46, 0.31, and 0.23, while the prefix “the model” is followed by the words “parameters” or “is” or a period, with frequencies 0.43, 0.36, and 0.21, and so forth.\n",
    "\n",
    "In an n-gram language model, the probability of a token given a prefix of n-1 tokens is given by its maximum likelihood estimate (MLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544792aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set some global parameters\n",
    "\n",
    "# Displaying all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# We will work with trigrams \n",
    "ngrams_degree = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e507a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Split the dataset into a training and a testing subset. \n",
    "\n",
    "Use the category “title” for the testing set and the categories “comment” and “post” for the training set. The short length of titles will make them good candidates later as seeds for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59618ecd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/stackexchange_cleaned.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d45b47",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd0fa3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc746d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055ae08",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda t : tokenizer.tokenize(' '.join(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208d728",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.sample(5).tokens.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920ea97",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into train and testing subset\n",
    "df_train = df[df.category.isin(['post', 'comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cb316",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Display the dimensions of the dataframe \n",
    "print(\"-- Training set: {}\\n\".format(df_train.shape))\n",
    "# and the 1st 5 lines\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n-- Testing set {}\\n\".format(df_test.shape))\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2b7b7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Build the matrix of prefix—word frequencies.\n",
    "\n",
    "- Use the ngrams function from nltk.utils to generate all n-grams from the corpus.\n",
    "\n",
    "\n",
    "- Set the following: left_pad_symbol = \\<s> and right_pad_symbol = \\</s>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30539639",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Counting bigrams and following tokens\n",
    "\n",
    "We build a counts object defined as a defaultdict(Counter). \n",
    "\n",
    "Taking into account all trigrams (ngrams_degree = 3) that we break into prefix (bigrams) followed by single tokens. \n",
    "\n",
    "The counts object will have the bigrams as keys and for each key a Counter of all the potential tokens. \n",
    "\n",
    "For instance, if the corpus contains a 100 instances of \"*how many people*\" and a 120 instances of \"*how many times*\" we would get the following entry:\n",
    "\n",
    "    counts[('how', 'many')] = Counter('people': 100, 'times': 120, .... )\n",
    "\n",
    "Similarly if the corpus contains \"*the model is*\" 500 times and \"*the model parameters*\" 200 times, we end up with:\n",
    "\n",
    "    counts[('the', 'model')] = Counter('is': 500, 'parameters': 200, .... )\n",
    "\n",
    "To split the tokens into bigramns we use the [ntlk.ngrams](https://www.nltk.org/api/nltk.html#nltk.util.ngrams) function:\n",
    "\n",
    "\n",
    "    Return the ngrams generated from a sequence of items, as an iterator.\n",
    "    For example:\n",
    "\n",
    "    >>> from nltk.util import ngrams\n",
    "    >>> list(ngrams([1,2,3,4,5], 3))\n",
    "    [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
    "\n",
    "The next cell should take a couple of minutes.\n",
    "\n",
    "Note that we build the mode on the training subset df_train and leave the testing subset aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46619c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1fe85",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def n_grams(sentence, ngrams_degree=3):\n",
    "    return ngrams(\n",
    "        sentence, \n",
    "        n = ngrams_degree,  \n",
    "        pad_right = True, \n",
    "        pad_left = True, \n",
    "        left_pad_symbol = \"<s>\", \n",
    "        right_pad_symbol = \"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961d0ebc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "list(n_grams(sentence.split(), 4 ) )[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2928730",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts = defaultdict(Counter)\n",
    "for tokens in tqdm(df_train.tokens.values):\n",
    "    for ngram in n_grams(tokens):      \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        counts[prefix][token] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98074d26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"we have {} bigrams\".format(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88319b31",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,counts[prefix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a131e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens_count = [ len(v)   for k,v in counts.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f44df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "plt.hist(tokens_count, bins = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fb6c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigrams_with_single_tokens = [ k   for k,v in counts.items() if len(v) == 1 ]\n",
    "bigrams_with_two_tokens = [ k   for k,v in counts.items() if len(v) == 2 ]\n",
    "\n",
    "print(\"{} bigrams_with_single_tokens\".format(len(bigrams_with_single_tokens)))\n",
    "print(\"{} bigrams_with_two_tokens\".format(len(bigrams_with_two_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12dd324",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokens_dict = { k:len(v)   for k,v in counts.items() if len(v) > 10000 }\n",
    "tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab2fbf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for prefix, tokens in counts.items():\n",
    "    print(\"prefix=\", prefix, \"\\ntokens=\", tokens.most_common(10))\n",
    "    print(sum(counts[prefix].values()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065c350",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  token / prefix probabilities\n",
    "\n",
    "To obtain token / prefix probabilities using the Maximum Likelihood Estimator, we must simply normalize each (prefix - token) count by the total number of the prefix occurence. \n",
    "\n",
    "$$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}$$\n",
    "\n",
    "\n",
    "Keeping the same defaultdict(Counter) structure for the freq object, we should obtain something similar to \n",
    "\n",
    "\n",
    "    freq[('how', 'many')] = {'people': 0.14, 'times': 120, .... }\n",
    "\n",
    "with \n",
    "* p(people / how many) = c('how many people') / c('how many') \n",
    "* p(times / how many) = c('how many times') / c('how many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2c7ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "freq = defaultdict(dict)\n",
    "for prefix, tokens in counts.items():\n",
    "    total = sum(counts[prefix].values())\n",
    "    for token, count in tokens.items():\n",
    "        freq[prefix][token] = count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8def671",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    prefix = random.choice(list(freq.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,freq[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5217dc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Write a text generation function with the following features:\n",
    "\n",
    "- Takes a bigram as input and generates the next token\n",
    "\n",
    "\n",
    "- Iteratively slides the prefix over the generated text so that the new prefix includes the most recent token; generates the next token\n",
    "\n",
    "\n",
    "- To generate each next token, samples the list of words associated with the prefix using the probability distribution of the prefix\n",
    "\n",
    "\n",
    "- Stops the text generation when a certain number of words have been generated or the latest token is a \\</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c00371",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate(text, n_words = 40):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        probabilities = list(freq[prefix].values())\n",
    "        text += ' ' + np.random.choice(candidates, p = probabilities)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b5add",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tuple('the model'.split()[-ngrams_degree+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5ebcf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text      = 'the model'\n",
    "print()\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'that distribution'\n",
    "print(generate(text))\n",
    "\n",
    "print()\n",
    "text      = 'to determine'\n",
    "print(generate(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec1451",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Write a function that can estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n",
    "\n",
    "\n",
    "Split the sentence into trigrams and use the chain rule to calculate the probability of the sentence as a product of the bigrams—tokens probabilities.\n",
    "\n",
    "- Estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences.\n",
    "\n",
    "- Similar to the above process calculate the candidates and initial_probabilites from freq dictionary\n",
    "\n",
    "- Here we will modify the initial_probabilites using temperature and normalizing it to generate random candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba280e6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Temperature sampling\n",
    "\n",
    "As you may have noticed, for some bigrams, one particular token may be much more frequent than the others potential tokens. \n",
    "\n",
    "For instance:\n",
    "\n",
    "* ('building', 'machine'): \t{'learning': 0.875, 'classification': 0.125}\n",
    "\n",
    "when generating the next token based on the bigram \"*building machine*\", most of the times the word \"learning\" will be chosen instead of \"classification\".\n",
    "\n",
    "In order to compensate these imbalances and improve the chances of less frequent tokens to be chosen we can sample with temperature.\n",
    "\n",
    "In order to increase the randomness of the next token selection given a prefix, we can flatten the distribution using the temperature $$\\tau$$ to define a new probability distribution as such:\n",
    "\n",
    "$$f_{\\tau}(p_i) = \\frac{ p_i^{\\frac{1}{\\tau}} }{ \\sum_j p_j^{\\frac{1}{\\tau}} }$$\n",
    "\n",
    "See [this post](https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling) for a more in-depth explanation on temperature sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50e78c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_temp(text, temperature = 1, n_words = 30):\n",
    "    for i in range(n_words):\n",
    "        prefix = tuple(text.split()[-ngrams_degree+1:])\n",
    "        if len(freq[prefix]) == 0:\n",
    "            break\n",
    "        candidates  = list(freq[prefix].keys())\n",
    "        initial_probabilities = list(freq[prefix].values())\n",
    "        denom   = sum([p ** temperature for p in initial_probabilities])\n",
    "        probabilities  = [p ** temperature / denom  for p in initial_probabilities]\n",
    "        text  += ' ' + np.random.choice(candidates, p = probabilities)\n",
    "        if text.endswith('</s>'):\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da17e56",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text  = 'the model'\n",
    "# text  = 'to determine'\n",
    "# text  = 'not sure'\n",
    "\n",
    "for tau in [0.01, 0.5, 1, 3, 10]:\n",
    "    print(tau)\n",
    "    print(generate_temp(text, temperature = tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d617432",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Implement the perplexity scoring function for a given sentence and for the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185cc34",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's now implement a way to measure the quality of our model.\n",
    "\n",
    "The idea is to estimate the probability of a test sentence given our model. \n",
    "An uncommon sentence should be less probable than a common one.\n",
    "\n",
    "\n",
    "Notes : \n",
    "  1. At this point the sentence should exist in the corpus. Our model does not know yet how to handle out-of-vocabulary (OOV) bigrams, trigrams or tokens.\n",
    "  2. To avoid the problem of underflow caused by multiplying multiple very small floats, we work in the log space:\n",
    "\n",
    "So instead of calculating perplexity with (case ngrams_degree = 3):\n",
    " \n",
    "$$PP(w_{1},\\cdots, w_N) = ( \\prod_{i = 3}^{N} \\frac{1}{ p(w_i/ w_{i-2}w_{i-1} )} )^{\\frac{1}{N}}$$\n",
    "\n",
    "We compute\n",
    "\n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 3}^{N} \\log {p(w_i/ w_{i-2}w_{i-1}} } ) ]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626ec17",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def perplexity(sentence):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "\n",
    "    for ngram in n_grams(sentence): \n",
    "        try:\n",
    "            prefix = ngram[:ngrams_degree-1] \n",
    "            token = ngram[ngrams_degree-1]\n",
    "            logprob += np.log(freq[prefix][token])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return np.exp(- logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11a326",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"The function may only be linear in the region where the points were taken\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eca363",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Implement additive Laplace smoothing to give a non-zero probability to missing prefix—token combinations when calculating perplexity.\n",
    "\n",
    "### Out of Vocabulary (OOV) \n",
    "\n",
    "The main weakness of our model so far is that it does not know how to handle elements that are not already in the original corpus.\n",
    "\n",
    "Since both when generating text and when calculating perplexity we use the count of the prefix in the corpus, when that prefix is missing, the counts = 0  which causes problems with logs and divisions.\n",
    "\n",
    "To remediate to that problem we can artificially assign a probability (although a very low one) to missing ngrams and tokens.\n",
    "\n",
    "This method is called Laplace smoothing. It relies on calculating the frequency of a token / prefix with:\n",
    "\n",
    "$$ p(token / prefix) = \\frac{ count( prefix + token) + \\delta}{count(prefix) + \\delta \\times |N| }$$\n",
    "\n",
    "\n",
    "Where \n",
    "\n",
    "* N is the total number of prefixes in the model\n",
    "* delta is an arbitrary number \n",
    "\n",
    "When the prefix is missing from the original corpus, the probability of a token / prefix will now be:\n",
    "\n",
    "$$p(token / prefix) = \\frac{1} { | N |}$$\n",
    "\n",
    "Let's implement that perplexity with Laplace Smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d3ad6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perplexity_laplace(sentence, delta = 1):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(sentence)\n",
    "    logprob = 0\n",
    "    for ngram in n_grams(sentence): \n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        if prefix in list(counts.keys()):\n",
    "            total = sum(counts[prefix].values())\n",
    "            if token in counts[prefix].keys():\n",
    "                logprob += np.log((counts[prefix][token] + delta)/ (total + delta * N))\n",
    "            else:\n",
    "                logprob += np.log((delta) / (total + delta * N ))\n",
    "        else:\n",
    "            logprob += - np.log(N)\n",
    "  \n",
    "    return np.exp(- logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61c65e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate the perplexity of sentences that were not present in the original corpus.\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774bae00",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Calculate the perplexity of the language model on the test set composed of titles.\n",
    "\n",
    "Perplexity on the test corpus and sentence probability.\n",
    "\n",
    "How do we calculate the perplexity of a model on a test corpus?\n",
    "\n",
    "Let's say we have *m* sentences in the corpus, the perplexity of the corpus is given by \n",
    "\n",
    "$$ PP(Corpus) = P(S_1, \\cdots, S_m)^{-\\frac{1}{N}} $$\n",
    "\n",
    "We can assume that the sentences are independent\n",
    "\n",
    "$$ PP(Corpus) = (\\prod_{k = 1}^{m}  P(S_k))^{-\\frac{1}{N}} $$\n",
    "\n",
    "Which we calculate in the log space to avoid underflow\n",
    "\n",
    "$$ PP(Corpus) = \\exp ( -\\frac{1}{N} \\sum_{k = 1}^{m}  log(P(S_k)) $$\n",
    "\n",
    "So to calculate the perplexity on a test corpus we need to calculate the probability of each single sentence.\n",
    "\n",
    "The following function calculates the probability of a sentence. \n",
    "\n",
    "Instead of using laplace smoothing to deal with the missing bigrams and tokens, we will simply skip missing elements to make the function faster.\n",
    "Implementing laplace smoothing requires several extra conditions that are taking too much time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c36af7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sentence_log_probability(sentence, delta = 1, ngrams_degree = 3):\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    logprob = 0\n",
    "    for ngram in n_grams(sentence, ngrams_degree):\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        try:\n",
    "            logprob += np.log( freq[prefix][token] )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1c0d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus, ngrams_degree = 3):\n",
    "    # start by calculating the total number of tokens in the corpus\n",
    "    all_sentences = ' '.join(corpus)\n",
    "\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    N = len(tokens)\n",
    "\n",
    "    logprob = 0\n",
    "    probs = []\n",
    "    for sentence in tqdm(corpus):\n",
    "        lp = sentence_log_probability(sentence, ngrams_degree)\n",
    "        probs.append(lp)\n",
    "        if lp != np.inf:\n",
    "            logprob += lp\n",
    "        else:\n",
    "            print(lp)\n",
    "#     print(probs)        \n",
    "    print(logprob, N)\n",
    "    return np.exp( - logprob / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e9065",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The perplexity of a sample of 1000 titles\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593dbd26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# and the perplexity of the whole test corpus\n",
    "corpus_perplexity(df_test.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22982ac3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Try to improve the perplexity score of your model as follows:\n",
    "\n",
    "- Modify the preprocessing phase of the corpus.\n",
    "\n",
    "\n",
    "- Increase or decrease the number of tokens in the model (bigrams, 4-grams, and so on).\n",
    "\n",
    "\n",
    "- Vary the delta parameter in the additive Laplace smoothing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf0978",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Modify the preprocessing phase of the corpus.\n",
    "\n",
    "Not clear what should change, so not tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696ab53",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Increase or decrease the number of tokens in the model (bigrams, 4-grams, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e958b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The perplexity of a sample of 1000 titles and using 4-grams\n",
    "# original 3-gram score was -31728.947433616246\n",
    "# did perplexity increase?\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "corpus_perplexity(corpus, ngrams_degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac2f56",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# and the perplexity of the whole test corpus using 4-grams\n",
    "# original 3-gram score was -2671335.5473894435\n",
    "# did perplexity decrease?\n",
    "corpus_perplexity(df_test.text.values, ngrams_degree=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675108e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Vary the delta parameter in the additive Laplace smoothing step.\n",
    "\n",
    "Perplexity using delta value of 10:\n",
    "\n",
    "[perplexity 145.18] this model belongs on a different planet\n",
    "\n",
    "[perplexity 35.54] this question really belongs on a different site.\n",
    "\n",
    "Below using a delta value of 20 seems to reduce perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c6223",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate the perplexity of sentences that were not present in the original corpus.\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 20), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 20), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a21ff1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Building an n-gram language model using NLTK\n",
    "\n",
    "Since version 3.4 the nltk library includes a language model module.\n",
    "\n",
    "Let's install the right version of nltk. Feel free to install any version > 3.4.5. \n",
    "\n",
    "After running the pip install command below you will need to restart the runtime. This will erase all the local variables. So we will reload and prepare the dataset from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd149e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dca2cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "ngrams_degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebb763",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe, shuffle it and reset the index\n",
    "# ../data/stackexchange_812k.csv\n",
    "# ../data/stackexchange_cleaned.csv\n",
    "df = pd.read_csv('../data/stackexchange_812k.tokenized.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a35f54",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(lambda txt : txt.split())\n",
    "df_train = df[df.category.isin(['post','comment'])].copy()\n",
    "df_test = df[df.category.isin(['title'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c68acf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cca179",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# len(list(vocab)) = 50543343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839d8a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = MLE(ngrams_degree,vocabulary=Vocabulary(unk_cutoff = 20))\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(ngrams_degree, df_train.tokens.values)\n",
    "\n",
    "# fit the model\n",
    "model.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7f2b6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then you can use the perplexity and generate functions of the lm module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6db0d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.perplexity(ngrams(df_test.tokens.values[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef694c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(model.perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(model.perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"The function may only be linear in the region where the points were taken\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(model.perplexity(sentence), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b0500",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Signature: model.generate(num_words=1, text_seed=None, random_seed=None)\n",
    "\n",
    "Generate words from the model.\n",
    "\n",
    ":param int num_words: How many words to generate. By default 1.\n",
    "\n",
    ":param text_seed: Generation can be conditioned on preceding context.\n",
    "\n",
    ":param random_seed: A random seed or an instance of `random.Random`. If provided,\n",
    "makes the random sampling part of generation reproducible.\n",
    "\n",
    ":return: One (str) word or a list of words generated from model.\n",
    "\n",
    "Examples:\n",
    "\n",
    "\\>>> from nltk.lm import MLE\n",
    "\n",
    "\\>>> lm = MLE(2)\n",
    "\n",
    "\\>>> lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
    "\n",
    "\\>>> lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
    "\n",
    "\\>>> lm.generate(random_seed=3)\n",
    "\n",
    "\\'a'\n",
    "\n",
    "\\>>> lm.generate(text_seed=['a'])\n",
    "\n",
    "\\'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b249b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.generate(num_words=10, random_seed=2, text_seed=['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d36410",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "random_seed=2\n",
    "n_words=40\n",
    "text      = 'the model'\n",
    "print()\n",
    "print(text+' '+' '.join(model.generate(n_words,text_seed=text,random_seed=random_seed)))\n",
    "\n",
    "print()\n",
    "text      = 'that distribution'\n",
    "print(text+' '+' '.join(model.generate(n_words,text_seed=text,random_seed=random_seed)))\n",
    "\n",
    "print()\n",
    "text      = 'to determine'\n",
    "print(text+' '+' '.join(model.generate(n_words,text_seed=text,random_seed=random_seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fad75",
   "metadata": {},
   "source": [
    "# M3 Deep Learning Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1e879",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this milestone, we will build a language model using a long short-term memory (LSTM) neural network. The problem is framed as a multiclass classification problem where the number of classes corresponds to the size of the vocabulary. The number can be quite large. Given a sequence of n-grams, the classifier predicts the following token as a class. The input of the neural network is an array of sequences of tokens for the design matrix and the output is a vector of labels that corresponds to the target token.\n",
    "\n",
    "When the vocabulary size is too large, training the model takes too long and the performance degrades. The challenge, therefore, lies in finding the right balance between the feasibility of the task and the quality of the model by reducing the vocabulary size but preserving its diversity.\n",
    "\n",
    "Your goal is to create a language model that generates high-quality text with a low perplexity score on a validation set and is reasonably fast to train.\n",
    "\n",
    "Creating a deep learning token-based language model brings specific challenges. Successfully implementing and training such a model on a real-world dataset requires the following:\n",
    "\n",
    "- Optimizing Python structures and control flows to minimize memory impact and reduce processing times\n",
    "\n",
    "- Balancing data diversity and dataset reduction\n",
    "\n",
    "\n",
    "There are many parameters to handle both in the data processing and model fitting phases, and finding the right balance is also a challenge.\n",
    "\n",
    "The language model building approach is entirely different from the n-gram approach. Instead of estimating tokens’ probability distributions using a maximum likelihood approach (counting occurrences of tokens) as we did for the n-grams language model, we train a classification model using a recurrent neural network approach.\n",
    "\n",
    "There will be a comparison of the n-gram language model built in the previous milestone with this deep learning model, highlighting the strengths, weaknesses, and difficulties inherent to each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "# Note that we will not use keras tokenizer but keep using the same NLTK tokenizer from task 1\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# We use Keras here for simplicity. Replace with your neural network of choice.\n",
    "\n",
    "#Load Keras libraries\n",
    "\n",
    "# dataframe display option\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42e49a",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f02352",
   "metadata": {},
   "source": [
    "### Load the dataset that was prepared in Milestone 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fe836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup variables\n",
    "POSTS_TYPE = 'post'\n",
    "MIN_TOKEN_LENGTH = 100\n",
    "MAX_TOKEN_LENGTH = 200\n",
    "DF_SAMPLE_COUNT = 20000\n",
    "\n",
    "TOKENS_MIN_COUNT = 10\n",
    "\n",
    "SEQUENCE_WINDOW = 4\n",
    "SEQUENCE_LEN = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f99a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('../data/stackexchange_812k.tokenized.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17deedb7",
   "metadata": {},
   "source": [
    "### The original dataset is too large and needs to be reduced. To reduce it, you can, for instance, use the following techniques:\n",
    "\n",
    "- Filter out items that have too many or too few tokens.\n",
    "\n",
    "- Select items of a certain type, such as posts, comments, or titles.\n",
    "\n",
    "- Sub-sample items randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9378ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9eaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[\n",
    "            (df_full.category == POSTS_TYPE) & \n",
    "            (df_full.n_tokens > MIN_TOKEN_LENGTH)  & \n",
    "            (df_full.n_tokens < MAX_TOKEN_LENGTH)\n",
    "        ].sample(DF_SAMPLE_COUNT).reset_index(drop = True)\n",
    "\n",
    "print(\"df.shape: \", df.shape)\n",
    "print(df.text.sample(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547436f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the tokens field from white space separated strings into list of tokens\n",
    "df['tokens'] = df.tokens.apply(lambda t : np.array(t.split()))\n",
    "print(df.tokens.sample().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce85110",
   "metadata": {},
   "source": [
    "### Build the vocabulary as the set of all unique tokens to construct the list of token indexes.\n",
    "\n",
    "Filtering on token frequency is one way to reduce the overall size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate vocabulary\n",
    "#filter out words that are too scarce\n",
    "import itertools\n",
    "all_tokens = list(itertools.chain.from_iterable(df.tokens))\n",
    "\n",
    "#filter out least common tokens\n",
    "from collections import Counter\n",
    "counter_tokens = Counter(all_tokens)\n",
    "\n",
    "vocab_size  = len(set(all_tokens))\n",
    "vocab       = list(set(all_tokens))\n",
    "print(\"original number of tokens\", len(all_tokens))\n",
    "print(\"original vocab_size\", vocab_size)\n",
    "\n",
    "#remove all tokens that appear in less than TOKENS_MIN_COUNT times\n",
    "fltrd_tokens = [ token for token in all_tokens if counter_tokens[token] > TOKENS_MIN_COUNT ]\n",
    "\n",
    "print(\"new number of tokens\", len(fltrd_tokens))\n",
    "print(\"new vocab_size\", len(set(fltrd_tokens)))\n",
    "\n",
    "vocab_size  = len(set(fltrd_tokens))\n",
    "vocab       = list(set(fltrd_tokens))\n",
    "vocab.append('UNK')\n",
    "vocab_size +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcdc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejected tokens\n",
    "rejected_tokens =  [ token for token in all_tokens if counter_tokens[token] <= TOKENS_MIN_COUNT ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de791fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(rejected_tokens): \", len(rejected_tokens))\n",
    "print(np.random.choice(rejected_tokens, 100, replace = False))\n",
    "# len(rejected_tokens):  25497"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf994e7",
   "metadata": {},
   "source": [
    "### Set a fixed sequence length and build sequences of token indexes from the corpus. (See, for instance, Keras pad_sequences.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append('UNK')\n",
    "vocab_size +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2acb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = { w : i for i, w in enumerate(vocab) }\n",
    "\n",
    "def getidx(token):\n",
    "    try:\n",
    "        return mapping[token]\n",
    "    except:\n",
    "        return mapping['UNK']\n",
    "\n",
    "df['tokens_idx'] = df.tokens.apply(lambda tokens : np.array([getidx(token) for token in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357143cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tokens_idx.head(2).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ee3af",
   "metadata": {},
   "source": [
    "### Split the sequences into predictors and labels (keras.utils.to_categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83106b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most likely using tf version 2.9 - go back to 2.8 and original path works\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cea204",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [[1], [2, 3], [4, 5, 6]]\n",
    "pad_sequences(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d372de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences\n",
    "def generate_sequences(sentence):\n",
    "    sequences = []\n",
    "    _end = SEQUENCE_WINDOW\n",
    "    while _end < len(sentence) + SEQUENCE_WINDOW:\n",
    "        sequences.append(sentence[:_end])\n",
    "        _end += SEQUENCE_WINDOW\n",
    "    padded_seqs = pad_sequences(sequences, maxlen=SEQUENCE_LEN, padding='pre')\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d73a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sequence generation \n",
    "multi_sequences = df.tokens_idx.apply(generate_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ecdcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for d in tqdm(multi_sequences.values):\n",
    "    if i == 0:\n",
    "        all_sequences = d\n",
    "    else:\n",
    "        all_sequences = np.concatenate( ( all_sequences, d )  )\n",
    "    i +=1\n",
    "print(\"\\nsequences.shape: \",all_sequences.shape)\n",
    "# expected sequences.shape:  (722881, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample N% of the sequences to reduce the input dataset.\n",
    "if True:\n",
    "    mask = np.random.choice([False, True], len(all_sequences), p=[0.50, 0.50])\n",
    "    sequences = all_sequences[mask].copy()\n",
    "else:\n",
    "    sequences = all_sequences.copy()\n",
    "    print(\"\\nsequences.shape: \",sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the sequences into predictors and labels\n",
    "# tf.keras.utils.to_categorical\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d11c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_categorical([0, 1, 2, 3], num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the predictors and labels for the classificaton task.\n",
    "predictors  = sequences[:,:-1]\n",
    "label       = sequences[:,-1]\n",
    "\n",
    "print(\"predictors.shape\", predictors.shape)\n",
    "print(\"label.shape\", label.shape)\n",
    "\n",
    "# The to_categorical Keras function transforms the vocab_size vector of labels into a one hot encoded matrix of dimension (n, vocab_size)\n",
    "label_cat       = to_categorical(label, num_classes=vocab_size)\n",
    "\n",
    "print(\"label_cat.shape\", label_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e116b2",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "The data is now ready to be used to fit a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807597ee",
   "metadata": {},
   "source": [
    "### Define a simple sequential model with an embedding layer, LSTM(s), and a dense layer with softmax activation. \n",
    "\n",
    "Feel free to experiment with dropouts and different optimizers. Here, we would be focusing on Keras to perform language modeling in this milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43714188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6200151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define model\n",
    "an embedding dimension (32, 64, ...), \n",
    "2 LSTM layers \n",
    "followed by a dense layer with softmax activation\n",
    "the optimizer is RMSprop with a learning rate of 0.01\n",
    "'''\n",
    "embedding_dimension = 64\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(vocab_size,\n",
    "        embedding_dimension,\n",
    "        input_length=SEQUENCE_LEN -1)\n",
    "    )\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52991ad",
   "metadata": {},
   "source": [
    "### Specify the number of epochs, the batch size, and other fitting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46674209",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0715b62",
   "metadata": {},
   "source": [
    "### Fit the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a700c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model Fitting!\n",
    "'''\n",
    "model.fit(predictors, label_cat, batch_size = batch_size, epochs=epochs, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb73de",
   "metadata": {},
   "source": [
    "## Assessing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fccd9",
   "metadata": {},
   "source": [
    "### Write a function that generates text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ba117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to sample an index from a probability array\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f87721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(nmax, text, temperature):\n",
    "    n = 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    while (len(tokens) < nmax) :\n",
    "        n +=1\n",
    "        # only takes known words into account\n",
    "        tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "        # print(tokens_idx)\n",
    "        tokens_list = pad_sequences([tokens_idx], maxlen=SEQUENCE_LEN-1, padding='pre')\n",
    "        probas = model.predict(tokens_list, verbose=0)[0]\n",
    "        next_word_idx = sample(probas, temperature = temperature)\n",
    "        next_word = vocab[next_word_idx]\n",
    "        # print(next_word_idx, next_word)\n",
    "\n",
    "        # next_word = np.random.choice(vocab, p = probas)\n",
    "        if next_word != '?':\n",
    "            print(next_word, probas[vocab.index(next_word)]  )\n",
    "            text += ' ' + next_word\n",
    "        # print(text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if n> 200:\n",
    "            break;\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a432b",
   "metadata": {},
   "source": [
    "### Generate some text and take note of the following:\n",
    "- Token repetitions\n",
    "- Missing punctuation\n",
    "- Other anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(15, 'a random variable', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448f3a7",
   "metadata": {},
   "source": [
    "### Write a function that calculates the perplexity of a sentence and apply it to a subset of sentences to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29350404",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_WINDOW = 1\n",
    "\n",
    " # and define the perplexity for a sentence\n",
    "\n",
    "def perplexity(sentence):\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "    N = len(tokens)\n",
    "    # find the indexes of the tokens from the vocabulary\n",
    "    tokens_idx = [ vocab.index(word) if word in vocab else vocab.index('UNK') for word in tokens  ]\n",
    "    # generate a N x SEQUENCE_LEN array of padded sequences \n",
    "    sequences = generate_sequences(tokens_idx)\n",
    "    predictors  = sequences[:,:-1]\n",
    "    label       = sequences[:,-1]\n",
    "    # the probabilities of all the words in the vocab given each padded sequence\n",
    "    probas = model.predict(predictors, verbose=0)\n",
    "    # add the log of the probability of the label given the padded sequence\n",
    "    logprob = 0\n",
    "    for k in range(N):\n",
    "        p = probas[k,label[k]]\n",
    "        logprob += np.log( p  )    \n",
    "    return np.exp(- logprob / N), logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f35efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"In a fixed-effects model only time-varying variables can be used.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"I know a pretty little place in Southern California, down San Diego way.\"\n",
    "print(sentence, perplexity(sentence))\n",
    "\n",
    "sentence = \"This that is noon but yes apple whatever did regression variable\"\n",
    "print(sentence, perplexity(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22f750",
   "metadata": {},
   "source": [
    "### Define a validation set, such as 1,000 titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set\n",
    "df_valid = df_full[(df_full.category == 'title') & (df_full.n_tokens > 10)].sample(100, random_state = 88).reset_index(drop = True)\n",
    "print(\"df_valid\",df_valid.shape)\n",
    "print(df_valid.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f433907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_perplexity(corpus):\n",
    "    # start by calculating the total number of tokens in the corpus\n",
    "    all_sentences = ' '.join(corpus)\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    N = len(all_tokens)\n",
    "    logproba = 0\n",
    "    perps = []\n",
    "    for sentence in corpus:\n",
    "        pp, logp = perplexity(sentence)\n",
    "        logproba += logp\n",
    "        perps.append(pp)\n",
    "        print (\"{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{}\".format(pp, np.mean(perps), logp, logproba, np.exp( - logproba / (N  )), sentence  ))\n",
    "\n",
    "    return np.exp( - logproba / (N)), perps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1b2c6",
   "metadata": {},
   "source": [
    "### Transform that validation set into sequences of tokens using the training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Perplexity score on the validation set\n",
    "corpus = df_valid.tokens.values\n",
    "perplexity_score, scores = corpus_perplexity(corpus)\n",
    "print(\" Corpus perplexity: {:.2f}\".format(perplexity_score ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e218598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"perplexity scores of sentences - histogram\")\n",
    "plt.hist([sc for sc in scores if sc < 5000], bins=30);\n",
    "plt.xlabel('perplexity')\n",
    "plt.ylabel('# of sentences')\n",
    "plt.grid(alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96309ca",
   "metadata": {},
   "source": [
    "### Tune the neural net and the parameters of the preprocessing phase to improve the model’s perplexity score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644440b",
   "metadata": {},
   "source": [
    "# M4 Character-based Language Model with AllenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ac032",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this milestone, we will switch to character-based language models. We will implement the same deep learning multinomial classification approach that we completed in the previous milestone.\n",
    "\n",
    "The goal remains to predict the next token given a preceding sequence of tokens. However, by using characters as tokens, instead of words, we solve two problems:\n",
    "\n",
    "- There are no more out-of-vocabulary (OOV) tokens since all the characters are known in advance.\n",
    "\n",
    "- The total number of classes to predict is reduced to a few dozen characters instead of thousands of different words. (This is true for alphabetic-based scripts such as Latin, Arabic, or Cyrillic but not in the case of logographic scripts used in Mandarin, Korean, or Japanese.)\n",
    "\n",
    "To build a character-based language model on our domain-specific corpus, we will use the AllenNLP framework. AllenNLP is a state-of-the-art NLP framework created by the Allen Institute for AI. Its generic approach allows us to work on a wide range of NLP problems. AllenNLP is based on PyTorch.\n",
    "\n",
    "Experimenting with character-based language models underlines its differences compared to word-based models in terms of the implementation process and the resulting outputs (generated texts and perplexity scores). In particular, switching from words to characters as the target multiclass has two main advantages:\n",
    "\n",
    "- The number of target classes is drastically reduced from thousands of tokens to less than a hundred characters.\n",
    "\n",
    "- All characters are known in advance.\n",
    "\n",
    "The abstraction level of the AllenNLP framework makes it particularly well suited to handle all sorts of NLP tasks (POS, NER, and so on). And the investment required to learn the framework is well worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19e8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.nn import LSTM, Linear\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "from allennlp.data.fields import TextField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, CharacterTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary, DEFAULT_PADDING_TOKEN\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.training.gradient_descent_trainer import GradientDescentTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158c4100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\r\n"
     ]
    }
   ],
   "source": [
    "!cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2d7633",
   "metadata": {},
   "source": [
    "## 1. Explore and analyze the set of unique characters present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e8d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('../data/stackexchange_812k.tokenized.csv').sample(frac=1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0ddf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    return [char for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de07734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e', 'l', 'w'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(split_text('well'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ccd76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239219650\n",
      "<class 'str'>\n",
      "confusion about pooling layer, is it trainable or not?is your case somehow different from dozens of \n"
     ]
    }
   ],
   "source": [
    "#Concatenate all the original texts from the dataset and list the unique \n",
    "text = ''.join(df_full.text.values).lower()\n",
    "print(len(text))\n",
    "print(type(text))\n",
    "print(text[:100])\n",
    "# text.split()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334cc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = np.unique(np.array(split_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77aec6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\t', 1),\n",
       " ('\\x0b', 1),\n",
       " ('\\x0c', 1),\n",
       " (' ', 1),\n",
       " ('!', 1),\n",
       " (\"'\", 1),\n",
       " (',', 1),\n",
       " ('-', 1),\n",
       " ('.', 1),\n",
       " ('?', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split string into list of characters and use Counter from collections library to find most common characters\n",
    "\n",
    "char_count = Counter(all_characters)\n",
    "\n",
    "# Limit the count of characters to MAX_VOCAB_SIZE using char_count.most_common\n",
    "char_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788c132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the allowed characters to MAX_VOCAB_SIZE\n",
    "MAX_VOCAB_SIZE = 40\n",
    "valid_characters = [t[0] for t in  char_count.most_common(MAX_VOCAB_SIZE)]\n",
    "valid_characters.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10901e54",
   "metadata": {},
   "source": [
    "## 2. Subsample the dataset to take into account the titles only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b8aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTS_TYPE = 'title'\n",
    "DF_SAMPLE_COUNT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266545db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'parent_id', 'comment_id', 'text', 'category', 'tokens',\n",
       "       'n_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a74747ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:  (10000, 7)\n",
      "['Is gradient checking useless in high dimensional setting?'\n",
      " 'Linear regression what does the F statistic, R squared and residual standard error tell us?']\n"
     ]
    }
   ],
   "source": [
    "# subsample the original dataset\n",
    "\n",
    "df = df_full[df_full.category == POSTS_TYPE].sample(DF_SAMPLE_COUNT).reset_index(drop=True)\n",
    "\n",
    "print(\"df.shape: \", df.shape)\n",
    "\n",
    "print(df.text.sample(2).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1712312",
   "metadata": {},
   "source": [
    "## 3. Implement the character tokenization of the dataset and transform the tokens into AllenNLP instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a4fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1f8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = df.text.apply(lambda txt : tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee661ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [D, o, e, s,  , S, p, e, a, r, m, a, n, ', s, ...\n",
       "1    [C, r, o, s, s, -, v, a, l, i, d, a, t, i, o, ...\n",
       "2    [H, e, l, p,  , s, o, l, v, i, n, g,  , f, o, ...\n",
       "3    [C, o, r, r, e, l, a, t, i, o, n,  , e, s, t, ...\n",
       "4    [T, e, c, h, n, i, q, u, e, s,  , a, n, d,  , ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "808ea531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an Instance for each list of token. \n",
    "# The function takes a list of tokens and an indexer as input and returns an instance composed of the input and output tokens.\n",
    "def tokens_to_instance(tokens: List[Token], token_indexers: Dict[str, TokenIndexer]):\n",
    "    tokens = list(tokens)\n",
    "    tokens.insert(0, Token(START_SYMBOL))\n",
    "    tokens.append(Token(END_SYMBOL))\n",
    "\n",
    "    input_field  = TextField(tokens[:-1], token_indexers)\n",
    "    output_field = TextField(tokens[1:], token_indexers)\n",
    "    return Instance({'input_tokens': input_field, 'output_tokens': output_field})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f377de",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "instances = [tokens_to_instance(tokens, token_indexers) for tokens in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a2e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = {char: 1 for char in valid_characters}\n",
    "vocab = Vocabulary({'tokens': token_counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844466bf",
   "metadata": {},
   "source": [
    "## 4. Design an RNN with AllenNLP that includes the following:\n",
    "- An embedding of the tokens\n",
    "- A seq2seq LSTM layer\n",
    "- A feed-forward layer that outputs a probability distribution of the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "257fb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4d159da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(Model):\n",
    "    def __init__(self,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 hidden_size: int,\n",
    "                 max_len: int,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "\n",
    "        self.embedder = embedder\n",
    "\n",
    "        # initialize a Seq2Seq encoder, LSTM\n",
    "        self.rnn = PytorchSeq2SeqWrapper(LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))\n",
    "        self.hidden2out = Linear(in_features=self.rnn.get_output_dim(), out_features=vocab.get_vocab_size('tokens'))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, input_tokens, output_tokens):\n",
    "        '''\n",
    "        This is the main process of the Model where the actual computation happens. \n",
    "        Each Instance is fed to the forward method. \n",
    "        It takes dicts of tensors as input, with same keys as the fields in your Instance (input_tokens, output_tokens)\n",
    "        It outputs the results of predicted tokens and the evaluation metrics as a dictionary. \n",
    "        '''\n",
    "\n",
    "        # code goes here\n",
    "        embeddings = self.embedder(input_tokens)\n",
    "        mask = get_text_field_mask(input_tokens)\n",
    "        rnn_hidden = self.rnn(embeddings, mask)\n",
    "        out_logits = self.hidden2out(rnn_hidden)\n",
    "#         print('FORWARD output tokens', type(output_tokens['tokens']['tokens']), output_tokens['tokens']['tokens'])\n",
    "        loss = sequence_cross_entropy_with_logits(out_logits, output_tokens['tokens']['tokens'], mask)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def generate(self) -> Tuple[List[Token], torch.tensor]:\n",
    "\n",
    "        # code goes here\n",
    "        start_symbol_idx = self.vocab.get_token_index(START_SYMBOL, 'tokens')\n",
    "        end_symbol_idx = self.vocab.get_token_index(END_SYMBOL, 'tokens')\n",
    "        padding_symbol_idx = self.vocab.get_token_index(DEFAULT_PADDING_TOKEN, 'tokens')\n",
    "        \n",
    "        log_likelihood = 0.\n",
    "        words = []\n",
    "        state = (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
    "        \n",
    "        word_idx = start_symbol_idx\n",
    "        \n",
    "        for i in range(self.max_len):\n",
    "            tokens = torch.tensor([[word_idx]])\n",
    "\n",
    "            embeddings = self.embedder({'tokens': tokens})\n",
    "            output, state = self.rnn._module(embeddings, state)\n",
    "            output = self.hidden2out(output)\n",
    "\n",
    "            log_prob = torch.log_softmax(output[0, 0], dim=0)\n",
    "\n",
    "            dist = torch.exp(log_prob)\n",
    "\n",
    "            word_idx = start_symbol_idx\n",
    "\n",
    "            while word_idx in {start_symbol_idx, padding_symbol_idx}:\n",
    "                word_idx = torch.multinomial(\n",
    "                    dist, num_samples=1, replacement=False).item()\n",
    "\n",
    "            log_likelihood += log_prob[word_idx]\n",
    "\n",
    "            if word_idx == end_symbol_idx:\n",
    "                break\n",
    "\n",
    "            token = Token(text=self.vocab.get_token_from_index(word_idx, 'tokens'))\n",
    "            words.append(token)\n",
    "        \n",
    "        return words, log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c488ed",
   "metadata": {},
   "source": [
    "## 5. Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "305391f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=EMBEDDING_SIZE)\n",
    "\n",
    "embedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "model = RNNLanguageModel(embedder=embedder, hidden_size=HIDDEN_SIZE, max_len=80, vocab=vocab)\n",
    "\n",
    "# device = torch.cuda.current_device()\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e80f1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.LongTensor([1., 1.]).view(([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63e298c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7730115",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = SimpleDataLoader(instances,BATCH_SIZE , shuffle=True)\n",
    "data_loader.index_with(vocab)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b398a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\r\n"
     ]
    }
   ],
   "source": [
    "!cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aadf8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c862453ada42dd9897d043fa471555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de9e8cac5ac4a9a8f166378bc72c8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f29058291c843bcb22b54d780a4117f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad47e1183c049178aee0b765c719db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55aabfbb71d44203ad1e571e135b7ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545c8cf67027491aac4e73d4a2c47084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d105af38cc704d6e9fc74be27b2d701a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b9c3e12cdf4e308f38df820dd59cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79d5d8e941f408ba6a00dc20585b2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1d76f972634e93978f71e86b958f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba11f613cee49ff833b2a70f4c4fb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1d6ed03dd041e4b374a69635fcb03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0b87be1edd4243a8842790ab1cab3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a3b3f3ff1f44db8430e811ff64fc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d355f7ea85a43fc9633ec496a8b3fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f18f2ed95834539aa420912a18fddc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13bd6e53a9b4213b349ad80a99762f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128dc61679b94f14991a9a3c82303ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0c682e3738486aa9fbc24de224752f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4be7d0e1b8e490e8db7b6d5ccfdfb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 19,\n",
       " 'peak_worker_0_memory_MB': 5036.76171875,\n",
       " 'peak_gpu_0_memory_MB': 0,\n",
       " 'training_duration': '0:37:18.079772',\n",
       " 'epoch': 19,\n",
       " 'training_loss': 0.9322054076798355,\n",
       " 'training_worker_0_memory_MB': 5036.76171875,\n",
       " 'training_gpu_0_memory_MB': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GradientDescentTrainer(model=model,\n",
    "                                data_loader=data_loader,\n",
    "                                optimizer=optimizer,\n",
    "                                num_epochs=20,\n",
    "                                cuda_device=-1)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e655aa",
   "metadata": {},
   "source": [
    "## 6. Evaluate the model by calculating the loss of some sentences and by generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d0c93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text: str, model: Model) -> float:\n",
    "    tokenizer = CharacterTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
    "    instance = tokens_to_instance(tokens, token_indexers)\n",
    "    output = model.forward_on_instance(instance)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26d5e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0635973}\n",
      "{'loss': 2.6857073}\n",
      "{'loss': 1.9752011}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In a fixed-effects model only time-varying variables can be used.\"\n",
    "predict(sentence, model)\n",
    "\n",
    "sentence = \"I know a pretty little place in Southern California, down San Diego way.\"\n",
    "predict(sentence, model)\n",
    "\n",
    "sentence = \"This that is noon but yes apple whatever did regression variable\"\n",
    "predict(sentence, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a995a8",
   "metadata": {},
   "source": [
    "Cannot resolve this error: Could not run 'aten::values' with arguments from the 'CPU' backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35089e35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::values' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::values' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1060 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:248 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2504 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10285 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24548/3365726140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24548/2262858864.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7-dev/envs/allennlp=2.8.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7-dev/envs/allennlp=2.8.0/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;31m# If there's only one tensor argument to the embedder, and we just have one tensor to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# embed, we can just pass in that tensor, without requiring a name match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mtoken_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# If there are multiple tensor arguments, we have to require matching names from the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::values' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::values' is only available for these backends: [SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n\nSparseCPU: registered at aten/src/ATen/RegisterSparseCPU.cpp:958 [kernel]\nSparseCUDA: registered at aten/src/ATen/RegisterSparseCUDA.cpp:1060 [kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterSparseCsrCPU.cpp:221 [kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterSparseCsrCUDA.cpp:248 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:2504 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:8931 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:10285 [kernel]\nUNKNOWN_TENSOR_TYPE_ID: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\nBatched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    tokens, _ = model.generate()\n",
    "    print(''.join(token.text for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e18ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
